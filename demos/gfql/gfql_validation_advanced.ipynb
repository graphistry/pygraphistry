{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced GFQL Validation Patterns\n",
    "\n",
    "Deep dive into complex GFQL validation scenarios, performance considerations, and advanced patterns.\n",
    "\n",
    "## Prerequisites\n",
    "- Complete the [GFQL Validation Fundamentals](./gfql_validation_fundamentals.ipynb) notebook\n",
    "- Experience writing GFQL queries\n",
    "- Understanding of graph traversal concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import graphistry\n",
    "\n",
    "from graphistry.compute.validate import (\n",
    "    validate_syntax,\n",
    "    validate_schema,\n",
    "    validate_query,\n",
    "    extract_schema_from_dataframes,\n",
    "    extract_schema_from_plottable\n",
    ")\n",
    "from graphistry.compute.ast import n, e_forward, e_reverse, e\n",
    "\n",
    "print(f\"PyGraphistry version: {graphistry.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Multi-Hop Queries\n",
    "\n",
    "Validate queries with multiple hops and complex traversal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset\n",
    "nodes_df = pd.DataFrame({\n",
    "    'id': range(1, 21),\n",
    "    'name': [f'Entity_{i}' for i in range(1, 21)],\n",
    "    'type': ['user', 'product', 'order', 'payment'] * 5,\n",
    "    'risk_score': np.random.uniform(0, 100, 20),\n",
    "    'created_at': pd.date_range('2024-01-01', periods=20, freq='D'),\n",
    "    'tags': [['premium'], ['sale'], [], ['urgent']] * 5\n",
    "})\n",
    "\n",
    "edges_df = pd.DataFrame({\n",
    "    'src': np.random.choice(range(1, 21), 50),\n",
    "    'dst': np.random.choice(range(1, 21), 50),\n",
    "    'rel_type': np.random.choice(['purchased', 'viewed', 'paid_for', 'related_to'], 50),\n",
    "    'weight': np.random.uniform(0, 1, 50),\n",
    "    'timestamp': pd.date_range('2024-01-01', periods=50, freq='H')\n",
    "})\n",
    "\n",
    "schema = extract_schema_from_dataframes(nodes_df, edges_df)\n",
    "print(f\"Dataset: {len(nodes_df)} nodes, {len(edges_df)} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-hop validation with bounded hops\n",
    "multi_hop_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\"type\": {\"eq\": \"user\"}}},\n",
    "    {\"type\": \"e_forward\", \"hops\": 2},  # 2-hop traversal\n",
    "    {\"type\": \"n\", \"filter\": {\"risk_score\": {\"gt\": 50}}}\n",
    "]\n",
    "\n",
    "issues = validate_query(multi_hop_query, nodes_df=nodes_df, edges_df=edges_df)\n",
    "print(\"2-hop traversal query:\")\n",
    "print(f\"Validation issues: {len(issues)}\")\n",
    "for issue in issues:\n",
    "    print(f\"- {issue.level}: {issue.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named operations for complex patterns\n",
    "named_ops_query = [\n",
    "    {\"type\": \"n\", \"name\": \"start_users\", \"filter\": {\"type\": {\"eq\": \"user\"}}},\n",
    "    {\"type\": \"e_forward\", \"filter\": {\"rel_type\": {\"eq\": \"purchased\"}}},\n",
    "    {\"type\": \"n\", \"name\": \"products\", \"filter\": {\"type\": {\"eq\": \"product\"}}},\n",
    "    {\"type\": \"e_reverse\", \"filter\": {\"rel_type\": {\"eq\": \"viewed\"}}},\n",
    "    {\"type\": \"n\", \"name\": \"viewers\"}\n",
    "]\n",
    "\n",
    "issues = validate_query(named_ops_query, nodes_df=nodes_df, edges_df=edges_df)\n",
    "print(\"Named operations query (find who viewed products that users purchased):\")\n",
    "print(f\"Validation issues: {len(issues)}\")\n",
    "if not issues:\n",
    "    print(\"✅ Complex pattern validated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path validation with alternating patterns\n",
    "path_query = [\n",
    "    {\"type\": \"n\"},\n",
    "    {\"type\": \"e_forward\"},\n",
    "    {\"type\": \"n\"},\n",
    "    {\"type\": \"e_reverse\"},\n",
    "    {\"type\": \"n\"},\n",
    "    {\"type\": \"e\"}, # Bidirectional\n",
    "    {\"type\": \"n\"}\n",
    "]\n",
    "\n",
    "issues = validate_syntax(path_query)\n",
    "print(\"Alternating path pattern:\")\n",
    "print(f\"Pattern length: {len(path_query)} operations\")\n",
    "print(f\"Validation issues: {len(issues)}\")\n",
    "print(\"✅ Valid path structure\" if not issues else \"❌ Invalid path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Predicates\n",
    "\n",
    "Complex filtering with temporal, nested, and custom predicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal predicates validation\n",
    "temporal_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\n",
    "        \"created_at\": {\n",
    "            \"gt\": {\"type\": \"datetime\", \"value\": \"2024-01-10T00:00:00Z\"}\n",
    "        }\n",
    "    }},\n",
    "    {\"type\": \"e_forward\", \"filter\": {\n",
    "        \"timestamp\": {\n",
    "            \"between\": [\n",
    "                {\"type\": \"datetime\", \"value\": \"2024-01-01T00:00:00Z\"},\n",
    "                {\"type\": \"datetime\", \"value\": \"2024-01-15T00:00:00Z\"}\n",
    "            ]\n",
    "        }\n",
    "    }}\n",
    "]\n",
    "\n",
    "issues = validate_query(temporal_query, nodes_df=nodes_df, edges_df=edges_df)\n",
    "print(\"Temporal predicates query:\")\n",
    "print(f\"Validation issues: {len(issues)}\")\n",
    "if not issues:\n",
    "    print(\"✅ Temporal predicates validated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested predicates with AND/OR logic\n",
    "nested_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\n",
    "        \"_and\": [\n",
    "            {\"type\": {\"in\": [\"user\", \"payment\"]}},\n",
    "            {\"_or\": [\n",
    "                {\"risk_score\": {\"gte\": 75}},\n",
    "                {\"tags\": {\"contains\": \"urgent\"}}\n",
    "            ]}\n",
    "        ]\n",
    "    }}\n",
    "]\n",
    "\n",
    "issues = validate_query(nested_query, nodes_df=nodes_df, edges_df=edges_df)\n",
    "print(\"Nested predicates query:\")\n",
    "print(\"Finding: (user OR payment) AND (high risk OR urgent)\")\n",
    "print(f\"Validation issues: {len(issues)}\")\n",
    "for issue in issues:\n",
    "    print(f\"- {issue.level}: {issue.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type-specific validation\n",
    "type_specific_queries = [\n",
    "    # Numeric predicates\n",
    "    [{\"type\": \"n\", \"filter\": {\"risk_score\": {\"between\": [25, 75]}}}],\n",
    "    \n",
    "    # String predicates  \n",
    "    [{\"type\": \"n\", \"filter\": {\"name\": {\"regex\": \"Entity_[1-5]$\"}}}],\n",
    "    \n",
    "    # Array predicates\n",
    "    [{\"type\": \"n\", \"filter\": {\"tags\": {\"is_empty\": False}}}],\n",
    "    \n",
    "    # Null checks\n",
    "    [{\"type\": \"n\", \"filter\": {\"name\": {\"is_null\": False}}}]\n",
    "]\n",
    "\n",
    "for i, query in enumerate(type_specific_queries):\n",
    "    issues = validate_query(query, nodes_df=nodes_df, edges_df=edges_df)\n",
    "    print(f\"Query {i+1}: {query[0]['filter']}\")\n",
    "    print(f\"  Valid: {'✅' if not issues else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Queries with Complex Filters\n",
    "\n",
    "Advanced edge filtering with source/destination constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge query with source/destination filters\n",
    "edge_filter_query = [\n",
    "    {\"type\": \"e\", \n",
    "     \"filter\": {\"rel_type\": {\"eq\": \"purchased\"}},\n",
    "     \"source_filter\": {\"type\": {\"eq\": \"user\"}},\n",
    "     \"destination_filter\": {\"type\": {\"eq\": \"product\"}}\n",
    "    }\n",
    "]\n",
    "\n",
    "issues = validate_query(edge_filter_query, nodes_df=nodes_df, edges_df=edges_df)\n",
    "print(\"Edge query with endpoint filters:\")\n",
    "print(\"Finding: purchased edges from users to products\")\n",
    "print(f\"Validation issues: {len(issues)}\")\n",
    "if not issues:\n",
    "    print(\"✅ Complex edge filters validated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex edge pattern with multiple constraints\n",
    "complex_edge_pattern = [\n",
    "    {\"type\": \"n\", \"filter\": {\"risk_score\": {\"gt\": 80}}},\n",
    "    {\"type\": \"e_forward\", \n",
    "     \"filter\": {\n",
    "         \"_and\": [\n",
    "             {\"weight\": {\"gte\": 0.5}},\n",
    "             {\"timestamp\": {\"gte\": {\"type\": \"datetime\", \"value\": \"2024-01-05T00:00:00Z\"}}}\n",
    "         ]\n",
    "     },\n",
    "     \"destination_filter\": {\"type\": {\"ne\": \"payment\"}}\n",
    "    },\n",
    "    {\"type\": \"n\"}\n",
    "]\n",
    "\n",
    "issues = validate_query(complex_edge_pattern, nodes_df=nodes_df, edges_df=edges_df)\n",
    "print(\"Complex edge pattern:\")\n",
    "print(\"Finding: High-risk entities with strong recent connections (not to payments)\")\n",
    "print(f\"Validation issues: {len(issues)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "Validate queries while considering performance implications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: bounded vs unbounded hops\n",
    "import time\n",
    "\n",
    "# Bounded hops (good performance)\n",
    "bounded_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\"id\": {\"eq\": 1}}},\n",
    "    {\"type\": \"e_forward\", \"hops\": 3},\n",
    "    {\"type\": \"n\"}\n",
    "]\n",
    "\n",
    "# Unbounded hops (potential performance issue)\n",
    "unbounded_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\"id\": {\"eq\": 1}}},\n",
    "    {\"type\": \"e_forward\"},  # No hops limit!\n",
    "    {\"type\": \"n\"}\n",
    "]\n",
    "\n",
    "# Validate bounded\n",
    "start = time.time()\n",
    "bounded_issues = validate_syntax(bounded_query)\n",
    "bounded_time = time.time() - start\n",
    "\n",
    "# Validate unbounded\n",
    "start = time.time()\n",
    "unbounded_issues = validate_syntax(unbounded_query)\n",
    "unbounded_time = time.time() - start\n",
    "\n",
    "print(\"Performance Analysis:\")\n",
    "print(f\"\\nBounded query (3 hops):\")\n",
    "print(f\"  Validation time: {bounded_time*1000:.2f}ms\")\n",
    "print(f\"  Issues: {len(bounded_issues)}\")\n",
    "\n",
    "print(f\"\\nUnbounded query:\")\n",
    "print(f\"  Validation time: {unbounded_time*1000:.2f}ms\")\n",
    "print(f\"  Issues: {len(unbounded_issues)}\")\n",
    "for issue in unbounded_issues:\n",
    "    if issue.level == \"warning\":\n",
    "        print(f\"  ⚠️  {issue.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate query complexity\n",
    "def estimate_query_complexity(query):\n",
    "    \"\"\"Estimate relative complexity of a query.\"\"\"\n",
    "    complexity = 0\n",
    "    \n",
    "    for op in query:\n",
    "        if op[\"type\"] in [\"e_forward\", \"e_reverse\", \"e\"]:\n",
    "            hops = op.get(\"hops\", float('inf'))\n",
    "            complexity += min(hops, 10) * 2  # Cap at 10 for estimation\n",
    "        \n",
    "        if \"filter\" in op:\n",
    "            # Complex filters add to complexity\n",
    "            if \"_and\" in op[\"filter\"] or \"_or\" in op[\"filter\"]:\n",
    "                complexity += 3\n",
    "            else:\n",
    "                complexity += 1\n",
    "    \n",
    "    return complexity\n",
    "\n",
    "# Test different query complexities\n",
    "queries = [\n",
    "    [{\"type\": \"n\"}, {\"type\": \"e_forward\", \"hops\": 1}, {\"type\": \"n\"}],\n",
    "    [{\"type\": \"n\"}, {\"type\": \"e_forward\", \"hops\": 5}, {\"type\": \"n\"}],\n",
    "    [{\"type\": \"n\", \"filter\": {\"_and\": [{\"type\": {\"eq\": \"user\"}}, {\"risk_score\": {\"gt\": 50}}]}},\n",
    "     {\"type\": \"e_forward\"}, {\"type\": \"n\"}],\n",
    "]\n",
    "\n",
    "for i, q in enumerate(queries):\n",
    "    complexity = estimate_query_complexity(q)\n",
    "    issues = validate_syntax(q)\n",
    "    print(f\"\\nQuery {i+1} complexity: {complexity}\")\n",
    "    print(f\"  Operations: {len(q)}\")\n",
    "    print(f\"  Has warnings: {'Yes' if any(i.level == 'warning' for i in issues) else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema Evolution\n",
    "\n",
    "Handle schema changes and maintain backwards compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original schema\n",
    "original_nodes = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['A', 'B', 'C'],\n",
    "    'value': [10, 20, 30]\n",
    "})\n",
    "\n",
    "# Evolved schema (renamed column, added column)\n",
    "evolved_nodes = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['A', 'B', 'C'],\n",
    "    'score': [10, 20, 30],  # 'value' renamed to 'score'\n",
    "    'category': ['X', 'Y', 'Z']  # New column\n",
    "})\n",
    "\n",
    "# Query using old schema\n",
    "legacy_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\"value\": {\"gte\": 15}}}\n",
    "]\n",
    "\n",
    "# Validate against both schemas\n",
    "print(\"Legacy query validation:\")\n",
    "print(f\"Query: {legacy_query}\")\n",
    "\n",
    "original_schema = extract_schema_from_dataframes(original_nodes, pd.DataFrame())\n",
    "evolved_schema = extract_schema_from_dataframes(evolved_nodes, pd.DataFrame())\n",
    "\n",
    "original_issues = validate_schema(legacy_query, original_schema)\n",
    "evolved_issues = validate_schema(legacy_query, evolved_schema)\n",
    "\n",
    "print(f\"\\nOriginal schema: {'✅ Valid' if not original_issues else '❌ Invalid'}\")\n",
    "print(f\"Evolved schema: {'✅ Valid' if not evolved_issues else '❌ Invalid'}\")\n",
    "\n",
    "if evolved_issues:\n",
    "    print(\"\\nMigration needed:\")\n",
    "    for issue in evolved_issues:\n",
    "        print(f\"  - {issue.message}\")\n",
    "        if issue.suggestion:\n",
    "            print(f\"    Suggestion: {issue.suggestion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backwards compatibility helper\n",
    "def create_compatible_query(query, column_mapping):\n",
    "    \"\"\"Update query to use new column names.\"\"\"\n",
    "    import copy\n",
    "    new_query = copy.deepcopy(query)\n",
    "    \n",
    "    for op in new_query:\n",
    "        if \"filter\" in op:\n",
    "            for old_col, new_col in column_mapping.items():\n",
    "                if old_col in op[\"filter\"]:\n",
    "                    op[\"filter\"][new_col] = op[\"filter\"].pop(old_col)\n",
    "    \n",
    "    return new_query\n",
    "\n",
    "# Update query for new schema\n",
    "column_mapping = {\"value\": \"score\"}\n",
    "updated_query = create_compatible_query(legacy_query, column_mapping)\n",
    "\n",
    "print(\"Updated query for new schema:\")\n",
    "print(f\"Before: {legacy_query}\")\n",
    "print(f\"After: {updated_query}\")\n",
    "\n",
    "# Validate updated query\n",
    "updated_issues = validate_schema(updated_query, evolved_schema)\n",
    "print(f\"\\nValidation: {'✅ Valid' if not updated_issues else '❌ Invalid'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Validation Logic\n",
    "\n",
    "Extend validation for domain-specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom validation for business rules\n",
    "def validate_business_rules(query, schema):\n",
    "    \"\"\"Add custom business rule validation.\"\"\"\n",
    "    custom_issues = []\n",
    "    \n",
    "    # Rule 1: Don't allow queries on sensitive columns without filters\n",
    "    sensitive_columns = ['risk_score', 'payment_id']\n",
    "    \n",
    "    for i, op in enumerate(query):\n",
    "        if op.get(\"type\") == \"n\" and \"filter\" not in op:\n",
    "            # Check if this could expose sensitive data\n",
    "            from graphistry.compute.validate import ValidationIssue\n",
    "            custom_issues.append(ValidationIssue(\n",
    "                level=\"warning\",\n",
    "                message=\"Unfiltered node query may expose sensitive data\",\n",
    "                operation_index=i,\n",
    "                suggestion=\"Add filters to limit data exposure\"\n",
    "            ))\n",
    "    \n",
    "    # Rule 2: Warn about expensive patterns\n",
    "    consecutive_edges = 0\n",
    "    for i, op in enumerate(query):\n",
    "        if op[\"type\"] in [\"e_forward\", \"e_reverse\", \"e\"]:\n",
    "            consecutive_edges += 1\n",
    "            if consecutive_edges > 2:\n",
    "                custom_issues.append(ValidationIssue(\n",
    "                    level=\"warning\",\n",
    "                    message=f\"Query has {consecutive_edges} consecutive edge operations\",\n",
    "                    operation_index=i,\n",
    "                    suggestion=\"Consider adding node filters between edge operations\"\n",
    "                ))\n",
    "        else:\n",
    "            consecutive_edges = 0\n",
    "    \n",
    "    return custom_issues\n",
    "\n",
    "# Test custom validation\n",
    "risky_query = [\n",
    "    {\"type\": \"n\"},  # No filter!\n",
    "    {\"type\": \"e_forward\"},\n",
    "    {\"type\": \"e_forward\"},\n",
    "    {\"type\": \"e_forward\"},  # Three consecutive edges\n",
    "    {\"type\": \"n\"}\n",
    "]\n",
    "\n",
    "# Standard validation\n",
    "standard_issues = validate_syntax(risky_query)\n",
    "print(f\"Standard validation: {len(standard_issues)} issues\")\n",
    "\n",
    "# Custom validation\n",
    "custom_issues = validate_business_rules(risky_query, schema)\n",
    "print(f\"\\nCustom validation: {len(custom_issues)} issues\")\n",
    "for issue in custom_issues:\n",
    "    print(f\"  - {issue.level}: {issue.message}\")\n",
    "    print(f\"    {issue.suggestion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific validation example\n",
    "def validate_security_query(query, schema):\n",
    "    \"\"\"Validate queries for security/compliance use cases.\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check for required audit fields\n",
    "    has_timestamp_filter = False\n",
    "    \n",
    "    for op in query:\n",
    "        if \"filter\" in op:\n",
    "            filters = op[\"filter\"]\n",
    "            if \"timestamp\" in filters or \"created_at\" in filters:\n",
    "                has_timestamp_filter = True\n",
    "                break\n",
    "    \n",
    "    if not has_timestamp_filter:\n",
    "        from graphistry.compute.validate import ValidationIssue\n",
    "        issues.append(ValidationIssue(\n",
    "            level=\"warning\",\n",
    "            message=\"Security queries should include time-based filters\",\n",
    "            suggestion=\"Add timestamp or created_at filter for audit compliance\"\n",
    "        ))\n",
    "    \n",
    "    return issues\n",
    "\n",
    "# Test security validation\n",
    "security_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\"type\": {\"eq\": \"payment\"}}},\n",
    "    {\"type\": \"e_forward\"},\n",
    "    {\"type\": \"n\", \"filter\": {\"risk_score\": {\"gt\": 90}}}\n",
    "]\n",
    "\n",
    "security_issues = validate_security_query(security_query, schema)\n",
    "print(\"Security validation for payment query:\")\n",
    "if security_issues:\n",
    "    for issue in security_issues:\n",
    "        print(f\"⚠️  {issue.message}\")\n",
    "        print(f\"   {issue.suggestion}\")\n",
    "else:\n",
    "    print(\"✅ Passes security validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Plottable\n",
    "\n",
    "Advanced validation using Plottable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Plottable and extract schema\n",
    "g = graphistry.nodes(nodes_df, 'id').edges(edges_df, 'src', 'dst')\n",
    "\n",
    "# Extract schema from Plottable\n",
    "plottable_schema = extract_schema_from_plottable(g)\n",
    "\n",
    "# Advanced query using Plottable schema\n",
    "advanced_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\n",
    "        \"_and\": [\n",
    "            {\"type\": {\"in\": [\"user\", \"payment\"]}},\n",
    "            {\"risk_score\": {\"between\": [70, 100]}}\n",
    "        ]\n",
    "    }},\n",
    "    {\"type\": \"e_forward\", \"filter\": {\n",
    "        \"rel_type\": {\"in\": [\"purchased\", \"paid_for\"]}\n",
    "    }},\n",
    "    {\"type\": \"n\", \"name\": \"targets\"}\n",
    "]\n",
    "\n",
    "# Validate using Plottable\n",
    "issues = validate_query(advanced_query, g._nodes, g._edges)\n",
    "print(\"Advanced query validation with Plottable:\")\n",
    "print(f\"Issues: {len(issues)}\")\n",
    "if not issues:\n",
    "    print(\"✅ Query validated successfully against Plottable schema!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "1. **Multi-hop queries**: Always specify hop limits for performance\n",
    "2. **Complex predicates**: Use nested AND/OR for sophisticated filtering\n",
    "3. **Schema evolution**: Plan for column changes with validation\n",
    "4. **Custom validation**: Extend for domain-specific requirements\n",
    "5. **Performance**: Consider query complexity during validation\n",
    "\n",
    "### Best Practices\n",
    "- ✅ Validate early and often during development\n",
    "- ✅ Use named operations for complex patterns\n",
    "- ✅ Add custom validation for business rules\n",
    "- ✅ Cache schemas for better performance\n",
    "- ✅ Monitor validation warnings in production\n",
    "\n",
    "### Next Steps\n",
    "- [GFQL Validation for LLMs](./gfql_validation_llm.ipynb) - AI integration\n",
    "- [Production Validation Patterns](./gfql_validation_production.ipynb) - Scale validation\n",
    "- [GFQL Documentation](https://docs.graphistry.com/gfql/) - Complete reference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}