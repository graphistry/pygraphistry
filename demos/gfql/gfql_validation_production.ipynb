{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFQL Validation in Production\n",
    "\n",
    "Production-ready patterns for GFQL validation in platform engineering and DevOps contexts.\n",
    "\n",
    "## Target Audience\n",
    "- Platform Engineers\n",
    "- DevOps Teams\n",
    "- Backend Developers\n",
    "- System Architects\n",
    "\n",
    "## What You'll Learn\n",
    "- Plottable integration for validation\n",
    "- Performance optimization and caching\n",
    "- Testing and CI/CD integration\n",
    "- Monitoring and observability\n",
    "- API endpoint validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production imports\n",
    "import time\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphistry\n",
    "\n",
    "from graphistry.compute.validate import (\n",
    "    validate_syntax,\n",
    "    validate_schema,\n",
    "    validate_query,\n",
    "    extract_schema_from_plottable,\n",
    "    ValidationIssue\n",
    ")\n",
    "\n",
    "print(f\"PyGraphistry version: {graphistry.__version__}\")\n",
    "print(\"Production validation patterns loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plottable Integration\n",
    "\n",
    "Seamlessly validate queries against Plottable objects in production workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create production-like dataset\n",
    "def create_production_data(num_nodes=10000, num_edges=50000):\n",
    "    \"\"\"Create realistic production dataset.\"\"\"\n",
    "    \n",
    "    nodes_df = pd.DataFrame({\n",
    "        'node_id': range(num_nodes),\n",
    "        'entity_type': np.random.choice(['user', 'device', 'transaction', 'merchant'], num_nodes),\n",
    "        'risk_score': np.random.uniform(0, 100, num_nodes),\n",
    "        'created_at': pd.date_range('2024-01-01', periods=num_nodes, freq='1min'),\n",
    "        'country': np.random.choice(['US', 'UK', 'CA', 'AU', 'JP'], num_nodes),\n",
    "        'status': np.random.choice(['active', 'inactive', 'suspended'], num_nodes)\n",
    "    })\n",
    "    \n",
    "    edges_df = pd.DataFrame({\n",
    "        'source': np.random.choice(range(num_nodes), num_edges),\n",
    "        'target': np.random.choice(range(num_nodes), num_edges),\n",
    "        'edge_type': np.random.choice(['transacted', 'connected', 'authorized', 'flagged'], num_edges),\n",
    "        'amount': np.random.uniform(10, 10000, num_edges),\n",
    "        'timestamp': pd.date_range('2024-01-01', periods=num_edges, freq='30s')\n",
    "    })\n",
    "    \n",
    "    return nodes_df, edges_df\n",
    "\n",
    "# Create Plottable\n",
    "nodes_df, edges_df = create_production_data(1000, 5000)\n",
    "g = graphistry.nodes(nodes_df, 'node_id').edges(edges_df, 'source', 'target')\n",
    "\n",
    "print(f\"Created Plottable with {len(nodes_df)} nodes and {len(edges_df)} edges\")\n",
    "print(f\"Node columns: {list(nodes_df.columns)}\")\n",
    "print(f\"Edge columns: {list(edges_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlottableValidator:\n",
    "    \"\"\"Production validator for Plottable objects.\"\"\"\n",
    "    \n",
    "    def __init__(self, plottable):\n",
    "        self.plottable = plottable\n",
    "        self.schema = extract_schema_from_plottable(plottable)\n",
    "        self._cache = {}\n",
    "    \n",
    "    def validate(self, query: List[Dict]) -> Tuple[bool, List[ValidationIssue]]:\n",
    "        \"\"\"Validate query against Plottable schema.\"\"\"\n",
    "        \n",
    "        # Check cache\n",
    "        query_hash = self._hash_query(query)\n",
    "        if query_hash in self._cache:\n",
    "            return self._cache[query_hash]\n",
    "        \n",
    "        # Validate\n",
    "        issues = validate_query(\n",
    "            query,\n",
    "            nodes_df=self.plottable._nodes,\n",
    "            edges_df=self.plottable._edges\n",
    "        )\n",
    "        \n",
    "        result = (len(issues) == 0, issues)\n",
    "        self._cache[query_hash] = result\n",
    "        return result\n",
    "    \n",
    "    def _hash_query(self, query: List[Dict]) -> str:\n",
    "        \"\"\"Create hash of query for caching.\"\"\"\n",
    "        query_str = json.dumps(query, sort_keys=True)\n",
    "        return hashlib.md5(query_str.encode()).hexdigest()\n",
    "    \n",
    "    def get_schema_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get schema information for documentation.\"\"\"\n",
    "        return {\n",
    "            \"node_columns\": list(self.schema.node_columns.keys()),\n",
    "            \"edge_columns\": list(self.schema.edge_columns.keys()),\n",
    "            \"node_types\": {k: str(v) for k, v in self.schema.node_columns.items()},\n",
    "            \"edge_types\": {k: str(v) for k, v in self.schema.edge_columns.items()}\n",
    "        }\n",
    "\n",
    "# Test PlottableValidator\n",
    "validator = PlottableValidator(g)\n",
    "\n",
    "# Valid query\n",
    "valid_query = [\n",
    "    {\"type\": \"n\", \"filter\": {\"entity_type\": {\"eq\": \"user\"}}},\n",
    "    {\"type\": \"e_forward\", \"filter\": {\"amount\": {\"gt\": 1000}}},\n",
    "    {\"type\": \"n\", \"filter\": {\"risk_score\": {\"gte\": 80}}}\n",
    "]\n",
    "\n",
    "is_valid, issues = validator.validate(valid_query)\n",
    "print(f\"Query valid: {is_valid}\")\n",
    "print(f\"Schema info: {json.dumps(validator.get_schema_info(), indent=2)[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance & Caching\n",
    "\n",
    "Optimize validation performance for high-throughput systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedSchemaValidator:\n",
    "    \"\"\"High-performance validator with schema caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_size=1000, ttl_seconds=3600):\n",
    "        self._schema_cache = {}\n",
    "        self._query_cache = lru_cache(maxsize=cache_size)(self._validate_uncached)\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.stats = {\n",
    "            \"cache_hits\": 0,\n",
    "            \"cache_misses\": 0,\n",
    "            \"total_validations\": 0\n",
    "        }\n",
    "    \n",
    "    def extract_and_cache_schema(self, dataset_id: str, nodes_df: pd.DataFrame, \n",
    "                                edges_df: pd.DataFrame):\n",
    "        \"\"\"Extract and cache schema with TTL.\"\"\"\n",
    "        from graphistry.compute.validate import extract_schema_from_dataframes\n",
    "        \n",
    "        schema = extract_schema_from_dataframes(nodes_df, edges_df)\n",
    "        self._schema_cache[dataset_id] = {\n",
    "            \"schema\": schema,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"node_count\": len(nodes_df),\n",
    "            \"edge_count\": len(edges_df)\n",
    "        }\n",
    "        return schema\n",
    "    \n",
    "    def get_cached_schema(self, dataset_id: str) -> Optional[Any]:\n",
    "        \"\"\"Get schema from cache if valid.\"\"\"\n",
    "        if dataset_id not in self._schema_cache:\n",
    "            return None\n",
    "        \n",
    "        cache_entry = self._schema_cache[dataset_id]\n",
    "        age = time.time() - cache_entry[\"timestamp\"]\n",
    "        \n",
    "        if age > self.ttl_seconds:\n",
    "            del self._schema_cache[dataset_id]\n",
    "            return None\n",
    "        \n",
    "        return cache_entry[\"schema\"]\n",
    "    \n",
    "    def _validate_uncached(self, query_json: str, schema) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate query (wrapped for LRU cache).\"\"\"\n",
    "        query = json.loads(query_json)\n",
    "        issues = validate_schema(query, schema)\n",
    "        return len(issues) == 0, json.dumps([self._issue_to_dict(i) for i in issues])\n",
    "    \n",
    "    def validate_with_cache(self, query: List[Dict], dataset_id: str, \n",
    "                           schema=None) -> Tuple[bool, List[Dict]]:\n",
    "        \"\"\"Validate with caching.\"\"\"\n",
    "        self.stats[\"total_validations\"] += 1\n",
    "        \n",
    "        # Get schema\n",
    "        if schema is None:\n",
    "            schema = self.get_cached_schema(dataset_id)\n",
    "            if schema is None:\n",
    "                raise ValueError(f\"No cached schema for dataset {dataset_id}\")\n",
    "        \n",
    "        # Check query cache\n",
    "        query_json = json.dumps(query, sort_keys=True)\n",
    "        \n",
    "        # Use cached validation\n",
    "        try:\n",
    "            is_valid, issues_json = self._query_cache(query_json, schema)\n",
    "            self.stats[\"cache_hits\"] += 1\n",
    "        except:\n",
    "            self.stats[\"cache_misses\"] += 1\n",
    "            raise\n",
    "        \n",
    "        return is_valid, json.loads(issues_json)\n",
    "    \n",
    "    def _issue_to_dict(self, issue: ValidationIssue) -> Dict:\n",
    "        return {\n",
    "            \"level\": issue.level,\n",
    "            \"message\": issue.message,\n",
    "            \"operation_index\": issue.operation_index,\n",
    "            \"field\": issue.field\n",
    "        }\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        hit_rate = (self.stats[\"cache_hits\"] / \n",
    "                   max(1, self.stats[\"total_validations\"])) * 100\n",
    "        \n",
    "        return {\n",
    "            **self.stats,\n",
    "            \"hit_rate\": f\"{hit_rate:.1f}%\",\n",
    "            \"cached_schemas\": len(self._schema_cache)\n",
    "        }\n",
    "\n",
    "# Test caching performance\n",
    "cached_validator = CachedSchemaValidator()\n",
    "\n",
    "# Cache schema\n",
    "schema = cached_validator.extract_and_cache_schema(\"prod_dataset_1\", nodes_df, edges_df)\n",
    "\n",
    "# Performance test\n",
    "test_queries = [\n",
    "    [{\"type\": \"n\", \"filter\": {\"entity_type\": {\"eq\": \"user\"}}}],\n",
    "    [{\"type\": \"n\", \"filter\": {\"risk_score\": {\"gt\": 50}}}],\n",
    "    [{\"type\": \"n\"}, {\"type\": \"e_forward\"}, {\"type\": \"n\"}]\n",
    "]\n",
    "\n",
    "# Run multiple times to test cache\n",
    "print(\"Performance test with caching:\")\n",
    "for round_num in range(3):\n",
    "    start = time.time()\n",
    "    \n",
    "    for _ in range(100):\n",
    "        for query in test_queries:\n",
    "            cached_validator.validate_with_cache(query, \"prod_dataset_1\", schema)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"Round {round_num + 1}: {elapsed:.3f}s for 300 validations\")\n",
    "\n",
    "print(f\"\\nCache stats: {json.dumps(cached_validator.get_stats(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch validation for efficiency\n",
    "def batch_validate_queries(queries: List[List[Dict]], plottable) -> Dict[str, Any]:\n",
    "    \"\"\"Validate multiple queries efficiently.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    schema = extract_schema_from_plottable(plottable)\n",
    "    \n",
    "    results = []\n",
    "    error_count = 0\n",
    "    warning_count = 0\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        issues = validate_query(\n",
    "            query,\n",
    "            nodes_df=plottable._nodes,\n",
    "            edges_df=plottable._edges\n",
    "        )\n",
    "        \n",
    "        errors = [iss for iss in issues if iss.level == \"error\"]\n",
    "        warnings = [iss for iss in issues if iss.level == \"warning\"]\n",
    "        \n",
    "        error_count += len(errors)\n",
    "        warning_count += len(warnings)\n",
    "        \n",
    "        results.append({\n",
    "            \"query_index\": i,\n",
    "            \"valid\": len(errors) == 0,\n",
    "            \"errors\": len(errors),\n",
    "            \"warnings\": len(warnings)\n",
    "        })\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        \"total_queries\": len(queries),\n",
    "        \"valid_queries\": sum(1 for r in results if r[\"valid\"]),\n",
    "        \"total_errors\": error_count,\n",
    "        \"total_warnings\": warning_count,\n",
    "        \"elapsed_seconds\": elapsed,\n",
    "        \"queries_per_second\": len(queries) / elapsed,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "# Test batch validation\n",
    "batch_queries = [\n",
    "    [{\"type\": \"n\", \"filter\": {\"entity_type\": {\"eq\": \"user\"}}}],\n",
    "    [{\"type\": \"n\", \"filter\": {\"invalid_col\": {\"eq\": \"value\"}}}],  # Invalid\n",
    "    [{\"type\": \"n\"}, {\"type\": \"e_forward\", \"hops\": 2}, {\"type\": \"n\"}],\n",
    "    [{\"type\": \"invalid_op\"}],  # Invalid\n",
    "] * 25  # 100 queries total\n",
    "\n",
    "batch_results = batch_validate_queries(batch_queries, g)\n",
    "print(f\"Batch validation results:\")\n",
    "print(json.dumps({k: v for k, v in batch_results.items() if k != \"results\"}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Patterns\n",
    "\n",
    "Unit and integration testing strategies for GFQL validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pytest fixtures and tests\n",
    "example_test_code = '''\n",
    "import pytest\n",
    "import pandas as pd\n",
    "from graphistry.compute.validate import validate_query, extract_schema_from_dataframes\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_data():\n",
    "    \"\"\"Fixture providing sample graph data.\"\"\"\n",
    "    nodes = pd.DataFrame({\n",
    "        'id': [1, 2, 3],\n",
    "        'type': ['A', 'B', 'A'],\n",
    "        'value': [10, 20, 30]\n",
    "    })\n",
    "    \n",
    "    edges = pd.DataFrame({\n",
    "        'src': [1, 2],\n",
    "        'dst': [2, 3],\n",
    "        'weight': [1.0, 2.0]\n",
    "    })\n",
    "    \n",
    "    return nodes, edges\n",
    "\n",
    "@pytest.fixture\n",
    "def schema(sample_data):\n",
    "    \"\"\"Fixture providing schema.\"\"\"\n",
    "    nodes, edges = sample_data\n",
    "    return extract_schema_from_dataframes(nodes, edges)\n",
    "\n",
    "class TestGFQLValidation:\n",
    "    \"\"\"Test suite for GFQL validation.\"\"\"\n",
    "    \n",
    "    def test_valid_query(self, sample_data):\n",
    "        \"\"\"Test validation of valid query.\"\"\"\n",
    "        nodes, edges = sample_data\n",
    "        query = [\n",
    "            {\"type\": \"n\", \"filter\": {\"type\": {\"eq\": \"A\"}}}\n",
    "        ]\n",
    "        \n",
    "        issues = validate_query(query, nodes, edges)\n",
    "        assert len(issues) == 0\n",
    "    \n",
    "    def test_invalid_column(self, sample_data):\n",
    "        \"\"\"Test detection of invalid column.\"\"\"\n",
    "        nodes, edges = sample_data\n",
    "        query = [\n",
    "            {\"type\": \"n\", \"filter\": {\"invalid\": {\"eq\": \"X\"}}}\n",
    "        ]\n",
    "        \n",
    "        issues = validate_query(query, nodes, edges)\n",
    "        assert len(issues) > 0\n",
    "        assert any(\"not found\" in issue.message for issue in issues)\n",
    "    \n",
    "    def test_performance(self, sample_data):\n",
    "        \"\"\"Test validation performance.\"\"\"\n",
    "        import time\n",
    "        nodes, edges = sample_data\n",
    "        query = [{\"type\": \"n\"}, {\"type\": \"e_forward\"}, {\"type\": \"n\"}]\n",
    "        \n",
    "        start = time.time()\n",
    "        for _ in range(100):\n",
    "            validate_query(query, nodes, edges)\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        # Should validate 100 queries in under 1 second\n",
    "        assert elapsed < 1.0\n",
    "'''\n",
    "\n",
    "print(\"Example pytest test suite:\")\n",
    "print(example_test_code)\n",
    "\n",
    "# Demonstrate test data generation\n",
    "def generate_test_cases() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Generate test cases for validation.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"name\": \"valid_simple_query\",\n",
    "            \"query\": [{\"type\": \"n\"}],\n",
    "            \"expected_valid\": True,\n",
    "            \"expected_errors\": 0\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"invalid_operation_type\",\n",
    "            \"query\": [{\"type\": \"nodes\"}],\n",
    "            \"expected_valid\": False,\n",
    "            \"expected_errors\": 1\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"orphaned_edge\",\n",
    "            \"query\": [{\"type\": \"e_forward\"}],\n",
    "            \"expected_valid\": True,  # Valid syntax but has warning\n",
    "            \"expected_warnings\": 1\n",
    "        }\n",
    "    ]\n",
    "\n",
    "test_cases = generate_test_cases()\n",
    "print(f\"\\nGenerated {len(test_cases)} test cases\")\n",
    "for tc in test_cases:\n",
    "    print(f\"  - {tc['name']}: expects valid={tc['expected_valid']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CI/CD Integration\n",
    "\n",
    "Integrate GFQL validation into continuous integration pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Actions workflow example\n",
    "github_actions_yaml = '''\n",
    "name: GFQL Query Validation\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - 'queries/**/*.json'\n",
    "      - 'src/**/*.py'\n",
    "\n",
    "jobs:\n",
    "  validate-queries:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install graphistry[ai]\n",
    "        pip install pytest\n",
    "    \n",
    "    - name: Validate GFQL queries\n",
    "      run: |\n",
    "        python scripts/validate_queries.py queries/\n",
    "    \n",
    "    - name: Run validation tests\n",
    "      run: |\n",
    "        pytest tests/test_gfql_validation.py -v\n",
    "    \n",
    "    - name: Upload validation report\n",
    "      if: failure()\n",
    "      uses: actions/upload-artifact@v3\n",
    "      with:\n",
    "        name: validation-errors\n",
    "        path: validation_report.json\n",
    "'''\n",
    "\n",
    "print(\"GitHub Actions workflow:\")\n",
    "print(github_actions_yaml)\n",
    "\n",
    "# Validation script for CI\n",
    "validation_script = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Validate GFQL queries in CI/CD pipeline.\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from graphistry.compute.validate import validate_syntax\n",
    "\n",
    "def validate_query_files(directory):\n",
    "    \"\"\"Validate all query files in directory.\"\"\"\n",
    "    \n",
    "    query_files = glob.glob(f\"{directory}/**/*.json\", recursive=True)\n",
    "    results = {\"total\": 0, \"passed\": 0, \"failed\": 0, \"errors\": []}\n",
    "    \n",
    "    for file_path in query_files:\n",
    "        results[\"total\"] += 1\n",
    "        \n",
    "        try:\n",
    "            with open(file_path) as f:\n",
    "                query = json.load(f)\n",
    "            \n",
    "            issues = validate_syntax(query)\n",
    "            \n",
    "            if not any(i.level == \"error\" for i in issues):\n",
    "                results[\"passed\"] += 1\n",
    "            else:\n",
    "                results[\"failed\"] += 1\n",
    "                results[\"errors\"].append({\n",
    "                    \"file\": file_path,\n",
    "                    \"issues\": [{\n",
    "                        \"level\": i.level,\n",
    "                        \"message\": i.message\n",
    "                    } for i in issues]\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            results[\"failed\"] += 1\n",
    "            results[\"errors\"].append({\n",
    "                \"file\": file_path,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "    \n",
    "    # Write report\n",
    "    with open(\"validation_report.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Exit with error if any queries failed\n",
    "    if results[\"failed\"] > 0:\n",
    "        print(f\"❌ {results['failed']} queries failed validation\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(f\"✅ All {results['total']} queries passed validation\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: validate_queries.py <directory>\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    validate_query_files(sys.argv[1])\n",
    "'''\n",
    "\n",
    "print(\"\\nValidation script for CI:\")\n",
    "print(validation_script[:800] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-commit hook example\n",
    "pre_commit_config = '''\n",
    "# .pre-commit-config.yaml\n",
    "repos:\n",
    "  - repo: local\n",
    "    hooks:\n",
    "      - id: validate-gfql\n",
    "        name: Validate GFQL Queries\n",
    "        entry: python scripts/validate_gfql_hook.py\n",
    "        language: system\n",
    "        files: '\\\\.(json|py)$'\n",
    "        pass_filenames: true\n",
    "'''\n",
    "\n",
    "# Pre-commit hook script\n",
    "def create_pre_commit_hook():\n",
    "    \"\"\"Create pre-commit hook for GFQL validation.\"\"\"\n",
    "    \n",
    "    hook_code = '''\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Pre-commit hook for GFQL validation.\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from graphistry.compute.validate import validate_syntax\n",
    "\n",
    "def extract_gfql_from_python(content):\n",
    "    \"\"\"Extract GFQL queries from Python code.\"\"\"\n",
    "    # Simple pattern matching for demonstration\n",
    "    pattern = r'query\\s*=\\s*(\\[.*?\\])'\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "    \n",
    "    queries = []\n",
    "    for match in matches:\n",
    "        try:\n",
    "            query = eval(match)  # Unsafe in production!\n",
    "            queries.append(query)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return queries\n",
    "\n",
    "def validate_file(filepath):\n",
    "    \"\"\"Validate GFQL in a file.\"\"\"\n",
    "    \n",
    "    if filepath.endswith('.json'):\n",
    "        with open(filepath) as f:\n",
    "            query = json.load(f)\n",
    "        queries = [query]\n",
    "    \n",
    "    elif filepath.endswith('.py'):\n",
    "        with open(filepath) as f:\n",
    "            content = f.read()\n",
    "        queries = extract_gfql_from_python(content)\n",
    "    \n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "    # Validate all queries\n",
    "    for query in queries:\n",
    "        issues = validate_syntax(query)\n",
    "        errors = [i for i in issues if i.level == \"error\"]\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"\\n❌ GFQL validation failed in {filepath}:\")\n",
    "            for error in errors:\n",
    "                print(f\"  - {error.message}\")\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    failed_files = []\n",
    "    \n",
    "    for filepath in sys.argv[1:]:\n",
    "        if not validate_file(filepath):\n",
    "            failed_files.append(filepath)\n",
    "    \n",
    "    if failed_files:\n",
    "        print(f\"\\n{len(failed_files)} file(s) have GFQL validation errors\")\n",
    "        sys.exit(1)\n",
    "'''\n",
    "    \n",
    "    return hook_code\n",
    "\n",
    "print(\"Pre-commit configuration:\")\n",
    "print(pre_commit_config)\n",
    "print(\"\\nPre-commit hook script preview:\")\n",
    "print(create_pre_commit_hook()[:600] + \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring & Logging\n",
    "\n",
    "Production monitoring patterns for GFQL validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "class ValidationMonitor:\n",
    "    \"\"\"Monitor GFQL validation in production.\"\"\"\n",
    "    \n",
    "    def __init__(self, logger=None):\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        self.metrics = {\n",
    "            \"total_validations\": 0,\n",
    "            \"validation_errors\": 0,\n",
    "            \"validation_warnings\": 0,\n",
    "            \"validation_time_ms\": [],\n",
    "            \"error_types\": {},\n",
    "            \"query_patterns\": {}\n",
    "        }\n",
    "    \n",
    "    def log_validation(self, query: List[Dict], issues: List[ValidationIssue], \n",
    "                      elapsed_ms: float, context: Dict[str, Any] = None):\n",
    "        \"\"\"Log validation event with metrics.\"\"\"\n",
    "        \n",
    "        self.metrics[\"total_validations\"] += 1\n",
    "        self.metrics[\"validation_time_ms\"].append(elapsed_ms)\n",
    "        \n",
    "        # Count errors and warnings\n",
    "        errors = [i for i in issues if i.level == \"error\"]\n",
    "        warnings = [i for i in issues if i.level == \"warning\"]\n",
    "        \n",
    "        if errors:\n",
    "            self.metrics[\"validation_errors\"] += 1\n",
    "        if warnings:\n",
    "            self.metrics[\"validation_warnings\"] += 1\n",
    "        \n",
    "        # Track error types\n",
    "        for issue in issues:\n",
    "            error_type = self._categorize_error(issue)\n",
    "            self.metrics[\"error_types\"][error_type] = \\\n",
    "                self.metrics[\"error_types\"].get(error_type, 0) + 1\n",
    "        \n",
    "        # Track query patterns\n",
    "        pattern = self._extract_pattern(query)\n",
    "        self.metrics[\"query_patterns\"][pattern] = \\\n",
    "            self.metrics[\"query_patterns\"].get(pattern, 0) + 1\n",
    "        \n",
    "        # Log event\n",
    "        log_data = {\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"validation_time_ms\": elapsed_ms,\n",
    "            \"errors\": len(errors),\n",
    "            \"warnings\": len(warnings),\n",
    "            \"query_operations\": len(query),\n",
    "            \"context\": context or {}\n",
    "        }\n",
    "        \n",
    "        if errors:\n",
    "            self.logger.error(f\"GFQL validation failed\", extra=log_data)\n",
    "        elif warnings:\n",
    "            self.logger.warning(f\"GFQL validation warnings\", extra=log_data)\n",
    "        else:\n",
    "            self.logger.info(f\"GFQL validation passed\", extra=log_data)\n",
    "    \n",
    "    def _categorize_error(self, issue: ValidationIssue) -> str:\n",
    "        \"\"\"Categorize error for metrics.\"\"\"\n",
    "        if \"Invalid operation type\" in issue.message:\n",
    "            return \"invalid_operation\"\n",
    "        elif \"Column\" in issue.message and \"not found\" in issue.message:\n",
    "            return \"column_not_found\"\n",
    "        elif \"Invalid filter\" in issue.message:\n",
    "            return \"invalid_filter\"\n",
    "        elif \"Invalid predicate\" in issue.message:\n",
    "            return \"invalid_predicate\"\n",
    "        else:\n",
    "            return \"other\"\n",
    "    \n",
    "    def _extract_pattern(self, query: List[Dict]) -> str:\n",
    "        \"\"\"Extract query pattern for tracking.\"\"\"\n",
    "        pattern_parts = []\n",
    "        for op in query:\n",
    "            op_type = op.get(\"type\", \"unknown\")\n",
    "            has_filter = \"filter\" in op\n",
    "            pattern_parts.append(f\"{op_type}{'[F]' if has_filter else ''}\")\n",
    "        return \"-\".join(pattern_parts)\n",
    "    \n",
    "    def get_metrics_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get metrics summary.\"\"\"\n",
    "        avg_time = sum(self.metrics[\"validation_time_ms\"]) / \\\n",
    "                  max(1, len(self.metrics[\"validation_time_ms\"]))\n",
    "        \n",
    "        return {\n",
    "            \"total_validations\": self.metrics[\"total_validations\"],\n",
    "            \"error_rate\": self.metrics[\"validation_errors\"] / \n",
    "                         max(1, self.metrics[\"total_validations\"]),\n",
    "            \"warning_rate\": self.metrics[\"validation_warnings\"] / \n",
    "                           max(1, self.metrics[\"total_validations\"]),\n",
    "            \"avg_validation_time_ms\": avg_time,\n",
    "            \"top_error_types\": sorted(\n",
    "                self.metrics[\"error_types\"].items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:5],\n",
    "            \"top_query_patterns\": sorted(\n",
    "                self.metrics[\"query_patterns\"].items(),\n",
    "                key=lambda x: x[1],\n",
    "                reverse=True\n",
    "            )[:5]\n",
    "        }\n",
    "\n",
    "# Test monitoring\n",
    "monitor = ValidationMonitor()\n",
    "\n",
    "# Simulate production validations\n",
    "test_scenarios = [\n",
    "    ([{\"type\": \"n\"}, {\"type\": \"e_forward\"}, {\"type\": \"n\"}], []),  # Valid\n",
    "    ([{\"type\": \"node\"}], [ValidationIssue(\"error\", \"Invalid operation type\")]),  # Error\n",
    "    ([{\"type\": \"n\", \"filter\": {\"missing\": {\"eq\": 1}}}], \n",
    "     [ValidationIssue(\"error\", \"Column 'missing' not found\")]),  # Schema error\n",
    "] * 10\n",
    "\n",
    "for query, issues in test_scenarios:\n",
    "    start = time.time()\n",
    "    # Simulate validation time\n",
    "    time.sleep(0.001)\n",
    "    elapsed_ms = (time.time() - start) * 1000\n",
    "    \n",
    "    monitor.log_validation(query, issues, elapsed_ms, \n",
    "                          context={\"user_id\": \"test123\", \"api_version\": \"v1\"})\n",
    "\n",
    "print(\"Monitoring Metrics Summary:\")\n",
    "print(json.dumps(monitor.get_metrics_summary(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Integration\n",
    "\n",
    "REST API endpoint validation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask API example\n",
    "flask_api_code = '''\n",
    "from flask import Flask, request, jsonify\n",
    "from graphistry.compute.validate import validate_syntax, validate_query\n",
    "import pandas as pd\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Cache for schemas\n",
    "schema_cache = {}\n",
    "\n",
    "@app.route('/api/v1/validate', methods=['POST'])\n",
    "def validate_gfql():\n",
    "    \"\"\"Validate GFQL query endpoint.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        # Extract query and optional dataset ID\n",
    "        query = data.get('query')\n",
    "        dataset_id = data.get('dataset_id')\n",
    "        validate_schema = data.get('validate_schema', False)\n",
    "        \n",
    "        if not query:\n",
    "            return jsonify({\n",
    "                'error': 'Missing query parameter'\n",
    "            }), 400\n",
    "        \n",
    "        # Syntax validation\n",
    "        syntax_issues = validate_syntax(query)\n",
    "        \n",
    "        # Schema validation if requested\n",
    "        schema_issues = []\n",
    "        if validate_schema and dataset_id:\n",
    "            schema = schema_cache.get(dataset_id)\n",
    "            if schema:\n",
    "                schema_issues = validate_schema(query, schema)\n",
    "        \n",
    "        # Combine issues\n",
    "        all_issues = syntax_issues + schema_issues\n",
    "        \n",
    "        # Format response\n",
    "        response = {\n",
    "            'valid': not any(i.level == 'error' for i in all_issues),\n",
    "            'issues': [{\n",
    "                'level': issue.level,\n",
    "                'message': issue.message,\n",
    "                'operation_index': issue.operation_index,\n",
    "                'field': issue.field,\n",
    "                'suggestion': issue.suggestion\n",
    "            } for issue in all_issues],\n",
    "            'metadata': {\n",
    "                'query_operations': len(query),\n",
    "                'syntax_checked': True,\n",
    "                'schema_checked': validate_schema and dataset_id is not None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return jsonify(response), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'Validation error: {str(e)}'\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/v1/schema/<dataset_id>', methods=['PUT'])\n",
    "def update_schema(dataset_id):\n",
    "    \"\"\"Update cached schema for dataset.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        # Extract node and edge schemas\n",
    "        node_columns = data.get('node_columns', {})\n",
    "        edge_columns = data.get('edge_columns', {})\n",
    "        \n",
    "        # Create and cache schema\n",
    "        from graphistry.compute.validate import Schema\n",
    "        schema = Schema(node_columns=node_columns, edge_columns=edge_columns)\n",
    "        schema_cache[dataset_id] = schema\n",
    "        \n",
    "        return jsonify({\n",
    "            'message': f'Schema updated for dataset {dataset_id}',\n",
    "            'node_columns': list(node_columns.keys()),\n",
    "            'edge_columns': list(edge_columns.keys())\n",
    "        }), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'Schema update error: {str(e)}'\n",
    "        }), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=5000)\n",
    "'''\n",
    "\n",
    "print(\"Flask API Example:\")\n",
    "print(flask_api_code[:1500] + \"\\n...\")\n",
    "\n",
    "# Example API client\n",
    "def create_api_client_example():\n",
    "    \"\"\"Create example API client code.\"\"\"\n",
    "    \n",
    "    return '''\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class GFQLValidationClient:\n",
    "    \"\"\"Client for GFQL validation API.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    def validate_query(self, query, dataset_id=None, validate_schema=False):\n",
    "        \"\"\"Validate GFQL query via API.\"\"\"\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/api/v1/validate\",\n",
    "            json={\n",
    "                \"query\": query,\n",
    "                \"dataset_id\": dataset_id,\n",
    "                \"validate_schema\": validate_schema\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    \n",
    "    def update_schema(self, dataset_id, node_columns, edge_columns):\n",
    "        \"\"\"Update schema for dataset.\"\"\"\n",
    "        \n",
    "        response = requests.put(\n",
    "            f\"{self.base_url}/api/v1/schema/{dataset_id}\",\n",
    "            json={\n",
    "                \"node_columns\": node_columns,\n",
    "                \"edge_columns\": edge_columns\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "# Usage example\n",
    "client = GFQLValidationClient(\"http://localhost:5000\")\n",
    "\n",
    "# Validate query\n",
    "result = client.validate_query(\n",
    "    query=[{\"type\": \"n\"}, {\"type\": \"e_forward\"}, {\"type\": \"n\"}],\n",
    "    dataset_id=\"prod_graph\",\n",
    "    validate_schema=True\n",
    ")\n",
    "\n",
    "if result[\"valid\"]:\n",
    "    print(\"✅ Query is valid\")\n",
    "else:\n",
    "    print(\"❌ Query has issues:\")\n",
    "    for issue in result[\"issues\"]:\n",
    "        print(f\"  - {issue['level']}: {issue['message']}\")\n",
    "'''\n",
    "\n",
    "print(\"\\nAPI Client Example:\")\n",
    "print(create_api_client_example())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Considerations\n",
    "\n",
    "Security best practices for production GFQL validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecureValidator:\n",
    "    \"\"\"Secure GFQL validator with rate limiting and sanitization.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_query_size=1000, max_operations=50, \n",
    "                 rate_limit_per_minute=100):\n",
    "        self.max_query_size = max_query_size\n",
    "        self.max_operations = max_operations\n",
    "        self.rate_limit_per_minute = rate_limit_per_minute\n",
    "        self._request_times = {}\n",
    "    \n",
    "    def validate_secure(self, query: List[Dict], user_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Validate with security checks.\"\"\"\n",
    "        \n",
    "        # Check rate limit\n",
    "        if not self._check_rate_limit(user_id):\n",
    "            return {\n",
    "                \"error\": \"Rate limit exceeded\",\n",
    "                \"retry_after_seconds\": 60\n",
    "            }\n",
    "        \n",
    "        # Check query size\n",
    "        query_str = json.dumps(query)\n",
    "        if len(query_str) > self.max_query_size:\n",
    "            return {\n",
    "                \"error\": f\"Query too large (max {self.max_query_size} chars)\"\n",
    "            }\n",
    "        \n",
    "        # Check operation count\n",
    "        if len(query) > self.max_operations:\n",
    "            return {\n",
    "                \"error\": f\"Too many operations (max {self.max_operations})\"\n",
    "            }\n",
    "        \n",
    "        # Sanitize query\n",
    "        sanitized_query = self._sanitize_query(query)\n",
    "        \n",
    "        # Validate\n",
    "        try:\n",
    "            issues = validate_syntax(sanitized_query)\n",
    "            return {\n",
    "                \"valid\": not any(i.level == \"error\" for i in issues),\n",
    "                \"issues\": [{\"level\": i.level, \"message\": i.message} \n",
    "                          for i in issues]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            # Don't expose internal errors\n",
    "            return {\"error\": \"Validation failed\"}\n",
    "    \n",
    "    def _check_rate_limit(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if user is within rate limit.\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if user_id not in self._request_times:\n",
    "            self._request_times[user_id] = []\n",
    "        \n",
    "        # Remove old requests\n",
    "        self._request_times[user_id] = [\n",
    "            t for t in self._request_times[user_id] \n",
    "            if current_time - t < 60\n",
    "        ]\n",
    "        \n",
    "        # Check limit\n",
    "        if len(self._request_times[user_id]) >= self.rate_limit_per_minute:\n",
    "            return False\n",
    "        \n",
    "        self._request_times[user_id].append(current_time)\n",
    "        return True\n",
    "    \n",
    "    def _sanitize_query(self, query: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Sanitize query to prevent injection.\"\"\"\n",
    "        import copy\n",
    "        sanitized = copy.deepcopy(query)\n",
    "        \n",
    "        # Remove any potentially dangerous fields\n",
    "        dangerous_keys = ['__proto__', 'constructor', 'prototype']\n",
    "        \n",
    "        def clean_dict(d):\n",
    "            if isinstance(d, dict):\n",
    "                return {k: clean_dict(v) for k, v in d.items() \n",
    "                       if k not in dangerous_keys}\n",
    "            elif isinstance(d, list):\n",
    "                return [clean_dict(item) for item in d]\n",
    "            else:\n",
    "                return d\n",
    "        \n",
    "        return clean_dict(sanitized)\n",
    "\n",
    "# Test secure validation\n",
    "secure_validator = SecureValidator()\n",
    "\n",
    "# Test rate limiting\n",
    "user_id = \"test_user_123\"\n",
    "for i in range(5):\n",
    "    result = secure_validator.validate_secure(\n",
    "        [{\"type\": \"n\"}], \n",
    "        user_id\n",
    "    )\n",
    "    print(f\"Request {i+1}: {'Valid' if result.get('valid') else 'Error'}\")\n",
    "\n",
    "# Test query size limit\n",
    "large_query = [{\"type\": \"n\", \"filter\": {\"x\" * 100: {\"eq\": \"y\" * 100}}} for _ in range(20)]\n",
    "result = secure_validator.validate_secure(large_query, \"user2\")\n",
    "print(f\"\\nLarge query result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Best Practices\n",
    "\n",
    "### Production Checklist\n",
    "- ✅ **Plottable Integration**: Use `extract_schema_from_plottable()` for seamless validation\n",
    "- ✅ **Caching**: Implement schema and query result caching\n",
    "- ✅ **Batch Processing**: Validate multiple queries efficiently\n",
    "- ✅ **Testing**: Comprehensive test coverage with fixtures\n",
    "- ✅ **CI/CD**: Automated validation in pipelines\n",
    "- ✅ **Monitoring**: Track metrics and error patterns\n",
    "- ✅ **API Design**: RESTful endpoints with proper error handling\n",
    "- ✅ **Security**: Rate limiting, size limits, and sanitization\n",
    "\n",
    "### Performance Guidelines\n",
    "1. Cache schemas with appropriate TTL\n",
    "2. Use batch validation for multiple queries\n",
    "3. Implement connection pooling for API servers\n",
    "4. Monitor p95 validation times\n",
    "5. Set reasonable query size limits\n",
    "\n",
    "### Monitoring Metrics\n",
    "- Validation success/failure rates\n",
    "- Average validation time\n",
    "- Common error patterns\n",
    "- Cache hit rates\n",
    "- API response times\n",
    "\n",
    "### Next Steps\n",
    "1. Implement production validation service\n",
    "2. Set up monitoring dashboards\n",
    "3. Create runbooks for common issues\n",
    "4. Establish SLOs for validation performance\n",
    "5. Build automated alerting\n",
    "\n",
    "### Resources\n",
    "- [GFQL Documentation](https://docs.graphistry.com/gfql/)\n",
    "- [PyGraphistry API Reference](https://docs.graphistry.com/api/)\n",
    "- [Production Deployment Guide](https://docs.graphistry.com/deployment/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}