{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GFQL Call Operations\n",
    "\n",
    "This notebook demonstrates the Call operation in GFQL, which enables:\n",
    "- Invoking user-defined functions on graph data\n",
    "- Custom data transformations and enrichments\n",
    "- Integration with external services\n",
    "- Advanced analytics within graph queries\n",
    "\n",
    "**Security Note**: Call operations are restricted to a safelist of allowed functions for security."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import graphistry\n",
    "from graphistry import n, e_forward, e_reverse\n",
    "from graphistry.compute.ast import ASTCall, ASTLet, ASTChainRef\n",
    "\n",
    "# For convenience, use the alias\n",
    "from graphistry import call, let\n",
    "\n",
    "graphistry.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data: Network Traffic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample network traffic data\n",
    "edges_df = pd.DataFrame({\n",
    "    'src_ip': ['192.168.1.1', '192.168.1.1', '192.168.1.2', '10.0.0.1', '10.0.0.1', \n",
    "               '192.168.1.3', '192.168.1.3', '10.0.0.2', '172.16.0.1'],\n",
    "    'dst_ip': ['192.168.1.2', '10.0.0.1', '192.168.1.3', '192.168.1.3', '10.0.0.2',\n",
    "               '10.0.0.2', '172.16.0.1', '172.16.0.1', 'external.com'],\n",
    "    'protocol': ['HTTP', 'HTTPS', 'SSH', 'HTTP', 'DNS', 'HTTPS', 'SSH', 'HTTP', 'HTTPS'],\n",
    "    'bytes': [1024, 2048, 512, 4096, 128, 8192, 256, 16384, 32768],\n",
    "    'packets': [10, 20, 5, 40, 2, 80, 3, 160, 320],\n",
    "    'timestamp': pd.to_datetime([\n",
    "        '2024-01-01 10:00:00', '2024-01-01 10:05:00', '2024-01-01 10:10:00',\n",
    "        '2024-01-01 10:15:00', '2024-01-01 10:20:00', '2024-01-01 10:25:00',\n",
    "        '2024-01-01 10:30:00', '2024-01-01 10:35:00', '2024-01-01 10:40:00'\n",
    "    ])\n",
    "})\n",
    "\n",
    "nodes_df = pd.DataFrame({\n",
    "    'ip': ['192.168.1.1', '192.168.1.2', '192.168.1.3', '10.0.0.1', \n",
    "           '10.0.0.2', '172.16.0.1', 'external.com'],\n",
    "    'type': ['workstation', 'workstation', 'server', 'gateway', \n",
    "             'dns_server', 'web_server', 'external'],\n",
    "    'risk_level': [0.2, 0.3, 0.7, 0.5, 0.4, 0.8, 0.9]\n",
    "})\n",
    "\n",
    "g = graphistry.edges(edges_df, 'src_ip', 'dst_ip').nodes(nodes_df, 'ip')\n",
    "print(f\"Graph has {len(g._nodes)} nodes and {len(g._edges)} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Call Operations\n",
    "\n",
    "Call operations allow you to invoke functions to transform or analyze data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some analysis functions (these would be in the safelist)\n",
    "def calculate_traffic_score(df):\n",
    "    \"\"\"Calculate a traffic anomaly score based on bytes and packets.\"\"\"\n",
    "    if 'bytes' in df.columns and 'packets' in df.columns:\n",
    "        df['traffic_score'] = (\n",
    "            (df['bytes'] / df['bytes'].mean()) + \n",
    "            (df['packets'] / df['packets'].mean())\n",
    "        ) / 2\n",
    "    return df\n",
    "\n",
    "def classify_protocol_risk(df):\n",
    "    \"\"\"Classify protocols by risk level.\"\"\"\n",
    "    risk_map = {\n",
    "        'HTTP': 'medium',\n",
    "        'HTTPS': 'low',\n",
    "        'SSH': 'low',\n",
    "        'DNS': 'low',\n",
    "        'FTP': 'high',\n",
    "        'TELNET': 'critical'\n",
    "    }\n",
    "    if 'protocol' in df.columns:\n",
    "        df['protocol_risk'] = df['protocol'].map(risk_map).fillna('unknown')\n",
    "    return df\n",
    "\n",
    "# Use Call to apply transformations\n",
    "# Note: In production, function names must be in the safelist\n",
    "enriched = g.gfql([\n",
    "    # Apply traffic scoring to all edges\n",
    "    call('calculate_traffic_score', target='edges'),\n",
    "    \n",
    "    # Apply protocol risk classification\n",
    "    call('classify_protocol_risk', target='edges')\n",
    "])\n",
    "\n",
    "print(\"Enriched edges:\")\n",
    "print(enriched._edges[['src_ip', 'dst_ip', 'protocol', 'traffic_score', 'protocol_risk']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call with Arguments\n",
    "\n",
    "Call operations can accept arguments to customize their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_threshold(df, column, threshold, operation='gt'):\n",
    "    \"\"\"Filter dataframe by threshold.\"\"\"\n",
    "    if column in df.columns:\n",
    "        if operation == 'gt':\n",
    "            return df[df[column] > threshold]\n",
    "        elif operation == 'lt':\n",
    "            return df[df[column] < threshold]\n",
    "        elif operation == 'gte':\n",
    "            return df[df[column] >= threshold]\n",
    "        elif operation == 'lte':\n",
    "            return df[df[column] <= threshold]\n",
    "    return df\n",
    "\n",
    "# Filter high-traffic connections\n",
    "high_traffic = g.gfql([\n",
    "    # First calculate traffic scores\n",
    "    call('calculate_traffic_score', target='edges'),\n",
    "    \n",
    "    # Then filter for high scores\n",
    "    call('filter_by_threshold', \n",
    "         target='edges',\n",
    "         args={'column': 'traffic_score', 'threshold': 1.5, 'operation': 'gt'})\n",
    "])\n",
    "\n",
    "print(f\"High traffic connections: {len(high_traffic._edges)} edges\")\n",
    "print(high_traffic._edges[['src_ip', 'dst_ip', 'bytes', 'packets', 'traffic_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Call with Let Bindings\n",
    "\n",
    "Call operations work seamlessly with Let bindings for complex analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_centrality(g):\n",
    "    \"\"\"Calculate degree centrality for nodes.\"\"\"\n",
    "    # Count incoming and outgoing connections\n",
    "    in_degree = g._edges.groupby(g._destination).size().to_frame('in_degree')\n",
    "    out_degree = g._edges.groupby(g._source).size().to_frame('out_degree')\n",
    "    \n",
    "    # Merge with nodes\n",
    "    nodes = g._nodes.copy()\n",
    "    nodes = nodes.merge(in_degree, left_on=g._node, right_index=True, how='left')\n",
    "    nodes = nodes.merge(out_degree, left_on=g._node, right_index=True, how='left')\n",
    "    nodes['centrality'] = (nodes['in_degree'].fillna(0) + nodes['out_degree'].fillna(0)) / 2\n",
    "    \n",
    "    return g.nodes(nodes)\n",
    "\n",
    "# Complex analysis combining Let and Call\n",
    "analysis = let({\n",
    "    # Find high-risk nodes\n",
    "    'risky_nodes': n({'risk_level': lambda x: x > 0.6}),\n",
    "    \n",
    "    # Find their network\n",
    "    'risky_network': [\n",
    "        ASTChainRef('risky_nodes'),\n",
    "        e_forward(hops=2)\n",
    "    ],\n",
    "    \n",
    "    # Calculate centrality for the risky network\n",
    "    'analyzed_network': [\n",
    "        ASTChainRef('risky_network'),\n",
    "        call('calculate_node_centrality', target='graph')\n",
    "    ]\n",
    "})\n",
    "\n",
    "result = g.gfql([analysis])\n",
    "print(\"Nodes with centrality scores:\")\n",
    "print(result._nodes[['ip', 'type', 'risk_level', 'centrality']].sort_values('centrality', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call for Data Enrichment\n",
    "\n",
    "Call operations can enrich data with external information or complex calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_with_geolocation(df):\n",
    "    \"\"\"Simulate IP geolocation enrichment.\"\"\"\n",
    "    # In production, this might call an actual geolocation service\n",
    "    geo_data = {\n",
    "        '192.168.1.1': {'country': 'US', 'city': 'New York', 'lat': 40.7128, 'lon': -74.0060},\n",
    "        '192.168.1.2': {'country': 'US', 'city': 'New York', 'lat': 40.7128, 'lon': -74.0060},\n",
    "        '192.168.1.3': {'country': 'US', 'city': 'Boston', 'lat': 42.3601, 'lon': -71.0589},\n",
    "        '10.0.0.1': {'country': 'UK', 'city': 'London', 'lat': 51.5074, 'lon': -0.1278},\n",
    "        '10.0.0.2': {'country': 'UK', 'city': 'London', 'lat': 51.5074, 'lon': -0.1278},\n",
    "        '172.16.0.1': {'country': 'JP', 'city': 'Tokyo', 'lat': 35.6762, 'lon': 139.6503},\n",
    "        'external.com': {'country': 'CN', 'city': 'Beijing', 'lat': 39.9042, 'lon': 116.4074}\n",
    "    }\n",
    "    \n",
    "    if 'ip' in df.columns:\n",
    "        for col in ['country', 'city', 'lat', 'lon']:\n",
    "            df[col] = df['ip'].map(lambda x: geo_data.get(x, {}).get(col))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_geo_distance(df):\n",
    "    \"\"\"Calculate geographical distance for edges.\"\"\"\n",
    "    if all(col in df.columns for col in ['src_lat', 'src_lon', 'dst_lat', 'dst_lon']):\n",
    "        # Simplified distance calculation\n",
    "        df['geo_distance'] = np.sqrt(\n",
    "            (df['dst_lat'] - df['src_lat'])**2 + \n",
    "            (df['dst_lon'] - df['src_lon'])**2\n",
    "        ) * 111  # Rough conversion to km\n",
    "    return df\n",
    "\n",
    "# Enrich with geolocation data\n",
    "geo_enriched = g.gfql([\n",
    "    # Add geolocation to nodes\n",
    "    call('enrich_with_geolocation', target='nodes'),\n",
    "    \n",
    "    # Calculate geographical distances for edges\n",
    "    call('calculate_geo_distance', target='edges')\n",
    "])\n",
    "\n",
    "print(\"Nodes with geolocation:\")\n",
    "print(geo_enriched._nodes[['ip', 'type', 'country', 'city']].head())\n",
    "\n",
    "# Note: Edge distance calculation would require joining node geo data to edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Multi-Stage Analysis Pipeline\n",
    "\n",
    "Combine multiple Call operations in a complex analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(df, columns, method='zscore', threshold=2):\n",
    "    \"\"\"Detect anomalies in specified columns.\"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            if method == 'zscore':\n",
    "                mean = df[col].mean()\n",
    "                std = df[col].std()\n",
    "                df[f'{col}_anomaly'] = np.abs((df[col] - mean) / std) > threshold\n",
    "            elif method == 'iqr':\n",
    "                Q1 = df[col].quantile(0.25)\n",
    "                Q3 = df[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                df[f'{col}_anomaly'] = (\n",
    "                    (df[col] < (Q1 - 1.5 * IQR)) | \n",
    "                    (df[col] > (Q3 + 1.5 * IQR))\n",
    "                )\n",
    "    return df\n",
    "\n",
    "def aggregate_risk_scores(g):\n",
    "    \"\"\"Aggregate risk scores from edges to nodes.\"\"\"\n",
    "    if 'traffic_score' in g._edges.columns:\n",
    "        # Calculate max traffic score for each node\n",
    "        node_risk = g._edges.groupby(g._source)['traffic_score'].max().to_frame('max_traffic_score')\n",
    "        \n",
    "        nodes = g._nodes.copy()\n",
    "        nodes = nodes.merge(node_risk, left_on=g._node, right_index=True, how='left')\n",
    "        nodes['combined_risk'] = (\n",
    "            nodes['risk_level'] * 0.5 + \n",
    "            nodes['max_traffic_score'].fillna(0) * 0.5\n",
    "        )\n",
    "        \n",
    "        return g.nodes(nodes)\n",
    "    return g\n",
    "\n",
    "# Multi-stage security analysis pipeline\n",
    "security_analysis = g.gfql([\n",
    "    # Stage 1: Calculate traffic scores\n",
    "    call('calculate_traffic_score', target='edges'),\n",
    "    \n",
    "    # Stage 2: Classify protocol risks\n",
    "    call('classify_protocol_risk', target='edges'),\n",
    "    \n",
    "    # Stage 3: Detect anomalies in traffic\n",
    "    call('detect_anomalies', \n",
    "         target='edges',\n",
    "         args={'columns': ['bytes', 'packets'], 'method': 'zscore', 'threshold': 1.5}),\n",
    "    \n",
    "    # Stage 4: Aggregate risks to nodes\n",
    "    call('aggregate_risk_scores', target='graph'),\n",
    "    \n",
    "    # Stage 5: Filter for high-risk scenarios\n",
    "    n({'combined_risk': lambda x: x > 0.7}),\n",
    "    e_forward(),\n",
    "    n()\n",
    "])\n",
    "\n",
    "print(f\"Security analysis found {len(security_analysis._nodes)} nodes of interest\")\n",
    "print(\"\\nHigh-risk nodes:\")\n",
    "print(security_analysis._nodes[['ip', 'type', 'risk_level', 'combined_risk']].sort_values('combined_risk', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call with Custom Return Types\n",
    "\n",
    "Call operations can return different types of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_network(g):\n",
    "    \"\"\"Generate a summary report of the network.\"\"\"\n",
    "    summary = {\n",
    "        'node_count': len(g._nodes),\n",
    "        'edge_count': len(g._edges),\n",
    "        'node_types': g._nodes['type'].value_counts().to_dict() if 'type' in g._nodes else {},\n",
    "        'protocols': g._edges['protocol'].value_counts().to_dict() if 'protocol' in g._edges else {},\n",
    "        'avg_bytes': g._edges['bytes'].mean() if 'bytes' in g._edges else 0,\n",
    "        'total_traffic': g._edges['bytes'].sum() if 'bytes' in g._edges else 0\n",
    "    }\n",
    "    \n",
    "    # Add summary as graph metadata (in practice, might return separately)\n",
    "    g._metadata = summary\n",
    "    return g\n",
    "\n",
    "# Generate network summary\n",
    "summarized = g.gfql([\n",
    "    call('summarize_network', target='graph')\n",
    "])\n",
    "\n",
    "print(\"Network Summary:\")\n",
    "if hasattr(summarized, '_metadata'):\n",
    "    for key, value in summarized._metadata.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Security Considerations\n",
    "\n",
    "Call operations have important security features:\n",
    "\n",
    "1. **Safelist**: Only pre-approved functions can be called\n",
    "2. **Sandboxing**: Functions run in a restricted environment\n",
    "3. **Resource Limits**: Execution time and memory are bounded\n",
    "4. **Input Validation**: Arguments are validated before execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of safelist configuration (typically done at server level)\n",
    "SAFELIST = {\n",
    "    'calculate_traffic_score': {\n",
    "        'module': 'network_analysis',\n",
    "        'allowed_args': ['method'],\n",
    "        'timeout': 30,\n",
    "        'memory_limit': '1GB'\n",
    "    },\n",
    "    'classify_protocol_risk': {\n",
    "        'module': 'security_utils',\n",
    "        'allowed_args': [],\n",
    "        'timeout': 10\n",
    "    },\n",
    "    'filter_by_threshold': {\n",
    "        'module': 'data_filters',\n",
    "        'allowed_args': ['column', 'threshold', 'operation'],\n",
    "        'timeout': 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# Attempting to call non-safelisted function would raise an error\n",
    "try:\n",
    "    # This would fail in production if not in safelist\n",
    "    result = g.gfql([\n",
    "        call('dangerous_function', target='graph')\n",
    "    ])\n",
    "except Exception as e:\n",
    "    print(f\"Expected error: Function 'dangerous_function' not in safelist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Call operations in GFQL provide powerful capabilities for:\n",
    "\n",
    "1. **Data Transformation**: Apply complex transformations to graph data\n",
    "2. **Enrichment**: Add external data or calculated fields\n",
    "3. **Analysis**: Run sophisticated algorithms within queries\n",
    "4. **Integration**: Connect with external services and APIs\n",
    "5. **Pipelines**: Build multi-stage analysis workflows\n",
    "\n",
    "Key concepts:\n",
    "- `call(function_name, target='nodes'|'edges'|'graph', args={...})`\n",
    "- Functions must be in the server's safelist\n",
    "- Can be combined with Let bindings and other operations\n",
    "- Support for different return types and side effects\n",
    "- Security through sandboxing and resource limits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}