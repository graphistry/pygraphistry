{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d71900",
   "metadata": {},
   "source": [
    "________________\n",
    "# Jack's money went here. \n",
    "\n",
    "## Where is twitter likely to lean more and less now that  he's leaving? Where will there be matching donations?\n",
    "\n",
    "Jack Dorsey is pledging over 466 million dollars and wants matching donations. His rational is simple -- billionaires can spare a tithe to help communities and people, and compounded over a few hundred of his closest friends, have a tremendous impact. \n",
    "\n",
    "This dataset is based off of the tweet https://twitter.com/jack/status/1247616214769086465 which lists pledged organizations and their donation. \n",
    "__________________________\n",
    "### We will learn how to quickly data science this dataset. We will select feature representations and visualize the resulting graph using UMAP.\n",
    "\n",
    "Featurization is the foundation of datascience. Likewise, Graph Thinking requires edges between nodes. Many times the data we have from databases/dataframes is tabular and row like -- with no incling of an edge table. This does *not* have to be an impediment for *Graph Thinking and materialization of datascience workflows*. \n",
    "\n",
    "UMAP is a powerful tool that projects complex, heterogeneous data coming from potentially many different distributions, down to lower dimensional embeddings and projections. The embedding estimates similarity between the rows, or nodes of the data, and thus forms a graph. \n",
    "\n",
    "Standardizing a feature set across the databases used in every modern company and then sending it to UMAP serves as a powerful graph generation tool.  \n",
    "____________________________\n",
    "Here we demonstrate how to Featurize and use UMAP to generate implicit graphs. The features may then be used in subsequent modeling using your favorite libraries -- sklearn, tensorflow, pytorch[, geometric, lightening, ...], cuGraph, DGL, etc. We demonstrate 4 featurization methods -- (latent embeddings, transformer embeddings, ngrams embeddings, one-hot encodings) that may be mixed and used to make different features for different columns, automatically. \n",
    "\n",
    "Furthermore, when we `g.plot()` the results, it is layed out according to the 2-dimensional UMAP projection of the data -- nearness in that projection represents nearness in the resulting features. We will test this empiracally using the different featurization methods for textual, numeric and categorical data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a069ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install graphistry[ai]  # install the AI dependencies of Graphistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97443b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import graphistry\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461a22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90875a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphistry.register(api=3, protocol=\"https\", server=\"hub.graphistry.com\", username=os.environ['USERNAME'], password=os.environ['GRAPHISTRY_PASSWORD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb2823",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "We already added the dataset from the twitter link, downloading a copy (as of May 2022) from the google drive. We need to remove the first few rows to make a valid dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://gist.githubusercontent.com/silkspace/f8d7b8f279a5ffbd710c301fc402ec43/raw/95a722f5c65812322eaf085c1123b58d3ec3da3a/jack_donations.csv')\n",
    "df = df.fillna('')\n",
    "columns = df.iloc[3].values  \n",
    "ndf = pd.DataFrame(df[4:].values, columns=columns)\n",
    "ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52b4e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.Category.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454348e",
   "metadata": {},
   "source": [
    "# Create the Graph\n",
    "\n",
    "We will use `g.umap` to featurize and create edges. The details of how UMAP is able to create edges between rows in the data is beyond the scope of this tutorial, however, suffic it to say, it is automatically inferring a network of related entities based off of their column features. \n",
    "\n",
    "Here is the dataset as graph, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986ff93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = graphistry.nodes(ndf).bind(point_title='Category').umap()\n",
    "g.plot()  # fly around the clusters and click on nodes and edges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c8496",
   "metadata": {},
   "source": [
    "## The above featurized every column over the entire datase. Exploring the nodes and their nearest neighbors indeed clusters similar rows -- all in two lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76e628d",
   "metadata": {},
   "source": [
    "# Some light analysis and enrichment \n",
    "\n",
    "Lets convert Amount column into numeric, and then see who is getting what by category and grantee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ced06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ndf.columns\n",
    "ndf[' Amount ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8077b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's convert money into float money (get it?)\n",
    "from re import sub\n",
    "from decimal import Decimal\n",
    "\n",
    "def convert_money_string_to_float(money: str, return_float: bool = True):\n",
    "    value = Decimal(sub(r\"[^\\d\\-.]\", \"\", money))  # preserves minus signs\n",
    "    if return_float:\n",
    "        return float(value)\n",
    "    return value\n",
    "\n",
    "ndf['$ amount'] = ndf[' Amount '].apply(lambda x: convert_money_string_to_float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e0c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf['$ amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95f782",
   "metadata": {},
   "source": [
    "## Many of these categories are not distinct. But due to data coming in with different notation, it seems distinct. \n",
    "\n",
    "We will show in the next section how to deal with this by using the graphistry pipeline to convert the `Category` into a latent target that organizes the labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4fcbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_funding_by_category = ndf.groupby('Category')['$ amount'].sum()\n",
    "current_funding_by_category.map(lambda x: '${:3,}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b71456",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,7))\n",
    "current_funding_by_category.plot(kind='bar', rot=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382780f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grantees = ndf.groupby('Grantee')['$ amount'].sum()\n",
    "grants_sorted = grantees.sort_values()\n",
    "# top 10 recepients \n",
    "grants_sorted[-10:].map(lambda x: '${:3,}'.format(x))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# largest grants\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax= plt.subplot()\n",
    "# ax.set_xticks(range(len(label_list)))\n",
    "# ax.set_xticklabels(label_list, rotation=19)\n",
    "res = grants_sorted[-10:]\n",
    "\n",
    "res.plot(kind='bar', rot=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14330bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smallest grants\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "ax= plt.subplot()\n",
    "# ax.set_xticks(range(len(label_list)))\n",
    "# ax.set_xticklabels(label_list, rotation=19)\n",
    "res = grants_sorted[:10]\n",
    "\n",
    "res.plot(kind='bar', rot = 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad231a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Total Pledged ${:3,}'.format(current_funding_by_category.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217026ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this should be the same too\n",
    "'Total Pledged ${:3,}'.format(grantees.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f59d83",
   "metadata": {},
   "source": [
    "## Notice that the Category labels are mixed and interwoven \n",
    "We will show how judicious choice of parameters can standardize it without having to do data cleaning or mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.Category.unique()  # seems like there are 4-6 topics here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0565e61",
   "metadata": {},
   "source": [
    "_______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2783bf",
   "metadata": {},
   "source": [
    "# Featurize II\n",
    "\n",
    "let's do it again and concentrate on a subset of the columns, to get a sense for the different ways to featurize named columns.\n",
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c6b5c7",
   "metadata": {},
   "source": [
    "In the following, we concentrate on the textual `Why?` column as it describes the row/entity in question. Further, we select `y='Category'` as a target variable, and will encode it using a Topic Model as well as standard One-Hot-Encoding.\n",
    "\n",
    "\n",
    "In the following we will show how to encode textual and categorical data using \n",
    "\n",
    "1) Topic Models\n",
    "\n",
    "2) Sentence Transformers\n",
    "\n",
    "3) Ngrams \n",
    "\n",
    "And see the resulting graphs. We will use the Topic label generated by `y='Category'` to color the graphs, as well as `$ amount` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2255d688",
   "metadata": {},
   "source": [
    "# Topic Model (latent-) features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8acdf66",
   "metadata": {},
   "source": [
    "We encode the data using Topic Models. This turns the textual features into latent vectors. Likewise, we can do the same for the target data. \n",
    "\n",
    "\n",
    "Notice that we set `cardinality_threshold_target` very low and `min_words` very high to force featurization as topic models rather than one-hot or topic encoded;\n",
    "1) encode target using a topic model, and set `n_topics_target` as the dimension of the latent target factorization. This choice is based on the fact that there are really only 4-6 or so distinct categories across the labels, but they are mixed together. The labels are in fact Hierarchical categories. We can use the topic model to find the lowest moments of this Hierarchical classification in the distributional sense. \n",
    "\n",
    "2) and like\n",
    "wise for the features `Why?`, and set `n_topics` as the dimension of the latent feature factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad1fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = graphistry.nodes(ndf).bind(point_title='Category')\n",
    "\n",
    "g2 = g.umap(X=['Why?'], y = ['Category'], \n",
    "            min_words=50000, # encode as topic model by setting min_words high\n",
    "            n_topics_target=4, # turn categories into a 4dim vector of regressive targets\n",
    "            n_topics=21, # latent embedding size \n",
    "            cardinality_threshold_target=2, # make sure that we throw targets into topic model over targets\n",
    "            ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9e6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2._node_encoder.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b650ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretend you have a minibatch of new data -- transform under the fit from the above\n",
    "new_df, new_y = ndf.sample(5), ndf.sample(5) # pd.DataFrame({'Category': ndf['Category'].sample(5)})\n",
    "a, b = g2.transform(new_df, new_y, kind='nodes')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5076e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(g2._node_target, aspect='auto', cmap='hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2._node_encoder.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2._node_encoder.y.plot(kind='bar', figsize=(15,7)) # easier to see than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likewise you can play with how many edges to include using,\n",
    "g2 = g2.filter_weighted_edges(scale=0.25)  # lower positive values of scale mean closer similarity \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6ae81",
   "metadata": {},
   "source": [
    "## We have featurized the data and also run UMAP, which projects the features into a 2-dimensional space while generating edges.\n",
    "\n",
    "Plotting the result shows the similarity between entities. It does a good job overall at clustering by topic. Click in and check out some nearby nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a970c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = g2._node_features \n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b09d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = g2._node_target  # we've reduced 22 columns into 5\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can inspect the topics from the column headers\n",
    "label_list = y.columns\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7396c76b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## and see them across rows of the data\n",
    "fig = plt.figure(figsize=(17,10))\n",
    "ax = plt.subplot()\n",
    "plt.imshow(y, aspect='auto', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.ylabel('row number of data')\n",
    "ax.set_xticks(range(len(label_list)))\n",
    "ax.set_xticklabels(label_list, rotation=39)\n",
    "print(f'See the abundance of the data in the latent vector of the corresponding targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd69ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the marginal in the category topic distribution\n",
    "y.sum(0).plot(kind='bar', ylabel='support across data', rot=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf88b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking at the above bar chart we may read off the most "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b817ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the category columns are supported by the data\n",
    "from collections import Counter\n",
    "tops = y.values.argmax(1)\n",
    "for topic_number in range(y.shape[1]):\n",
    "    indices = np.where(tops==topic_number)\n",
    "    top_category = Counter(ndf.loc[indices].Category)\n",
    "    print()\n",
    "    print('-'*50)\n",
    "    print(f'Topic {topic_number}: \\t\\t\\t\\t Evidence')\n",
    "    print(f'{y.columns[topic_number]}')\n",
    "    print('-'*35)\n",
    "    for t, c in top_category.most_common():\n",
    "        print(f'-- {t},    {c}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe62b1e",
   "metadata": {},
   "source": [
    "### We see that different spellings, spaces, etc or use of ;, , etc map to the same topic. This is a useful way to disambiguate when there are many similar categories without having to do a lot of data cleaning and prep.\n",
    "\n",
    "The choice of `n_topics_target` sets the prior on the Dirty_Cat GapEncoder used under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530cde56",
   "metadata": {},
   "source": [
    "## Let's add the Category Topic Number as a feature to help us visualize using the Histogram Feature of the Graphistry UI\n",
    "\n",
    "This reduces the naive one-hot-encoding of 22 columns down the the number set by the `n_topics_target=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b9ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5724c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2._nodes['topic'] = y.columns[tops]\n",
    "ndf['topic'] = y.columns[tops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad387dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2._nodes.topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486a47c",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "In the plot below, use the histogram feature on the bottom right of the UI to color by `topic`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46b0ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g3 = g2.bind(point_title='topic')\n",
    "g3.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets sum $$ across major topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad0af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sums = ndf.groupby('topic')['$ amount'].sum()\n",
    "topic_sums.sort_values()[::-1].apply(lambda x : '${:3,}'.format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f5eef",
   "metadata": {},
   "source": [
    "## hence we have Crisis Relief, Social Justice, Health Education Girls, and UBI occupying the main topics across the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94756034",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------\n",
    "# Let's move on to point 2) \n",
    "# Sentence Transformer Encodings\n",
    "\n",
    "To trigger the sentence encoder, just lower the `min_words` count (which previously we had set to higher than the number of words across the `Why?` column) to some small value or zero to force encoding any X=[..] columns, since it sets the minimum number of words to consider passing on to the (sentence, ngram) embedding pipelines.  \n",
    "\n",
    "Here, UMAP will work directly on the sentence transformer vector and expose a search interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ceacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = g.umap(X = ['Why?', 'Grantee'], y = 'Category', \n",
    "            min_words=0, \n",
    "            model_name ='paraphrase-MiniLM-L6-v2', \n",
    "            cardinality_threshold_target=2,\n",
    "            scale=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2.search('carbon neutral')[0][['Why?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a222ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'${:3,}'.format(g2.search('carbon neutral')[0]['$ amount'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbcec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2.search('sustainable homes and communities')[0][['Why?','$ amount']]#.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc5bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "'${:3,}'.format(g2.search('sustainable homes and communities')[0]['$ amount'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc28169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the queries landscape  -- paste url with .plot(render=False)\n",
    "g2.search_graph('sustainable homes and communities', scale=0.90, top_n=10).bind(point_title='Why?').plot(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9f793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or transform on new data as before\n",
    "a, b = g2.transform(new_df, new_y, kind='nodes')\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3033a4",
   "metadata": {},
   "source": [
    "## Clicking around to nearest neighbors demonstrates good semantic similarity, as seen by the Paraphrase Model `paraphrase-MiniLM-L6-v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb210c",
   "metadata": {},
   "source": [
    "## Suppose we wanted to add the Grantee column as a feature: \n",
    "To include it in the sentence transformer model, reduce the` min_words` threshold to include it. If we want the column `Grantee` to be encoded as a topic model, set `min_words` to between the average of `Why?` (higher) and `Grantee` (lower) and `$ amount` (which is just 1). This may seem a bit sloppy as an API, nevertheless useful across many datasets since if a column is truly categorical, its cardinality is usually well under that of a truly textual feature. Moreover, if you want all columns to be textually encoded, set `min_words=0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef2870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = g.umap(X = ['Why?', 'Grantee', '$ amount'], y = 'Category',\n",
    "            min_words=2,\n",
    "            model_name ='paraphrase-MiniLM-L6-v2',\n",
    "            use_scaler=None,\n",
    "           ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bdaa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2._node_encoder.text_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b61370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for fun, can we find outliers (which we know will be influenced by the numeric $ amount)\n",
    "from graphistry.outliers import detect_outliers\n",
    "\n",
    "# organized by amount\n",
    "embedding = g2._xy\n",
    "clfs, ax, fig = detect_outliers(embedding.values, name='Donations', contamination=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the different models\n",
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3bc17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g2.plot() # color/size the noded by `$ amount`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014b4e0",
   "metadata": {},
   "source": [
    "# Lastly, suppose we want a plain Ngrams model matrix, and for a change, one-hot-encode the target `Category`\n",
    "\n",
    "Set `use_ngrams = True`\n",
    "and set the `cardinality_threshold_target` > cardinality(`Category`).\n",
    "\n",
    "UMAP will work directly on the ngrams matrix, and any other feature column one may transform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = g.umap(X = ['Why?', 'Grantee'], y = 'Category', \n",
    "            use_ngrams=True, \n",
    "            ngram_range=(1,3), \n",
    "            min_df=2, \n",
    "            max_df=0.3,\n",
    "            cardinality_threshold_target=400\n",
    "           )  # this will one-hot-encode the target, as we have less than 400 total `categories`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e2683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3.bind(point_title='Category').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3._node_features  # a standard tfidf ngrams matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338010f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "g3._node_encoder.text_model  #sklearn pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7582131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## vocab size\n",
    "len(g3._node_encoder.text_model[0].vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e691b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or transform new data: \n",
    "emb, a, b = g2.transform_umap(new_df, new_y, kind='nodes')\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7b2c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we include the naive indicator variable for completeness.\n",
    "y = g3._node_target\n",
    "label_list = b.columns\n",
    "\n",
    "fig = plt.figure(figsize=(17,10))\n",
    "ax = plt.subplot()\n",
    "plt.imshow(y, aspect='auto', cmap='hot')\n",
    "plt.colorbar()\n",
    "plt.ylabel('row number of data')\n",
    "ax.set_xticks(range(len(label_list)))\n",
    "ax.set_xticklabels(label_list, rotation=49)\n",
    "print('Naive Indicator Variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83a920",
   "metadata": {},
   "source": [
    "# Contributions\n",
    "\n",
    "We've seen how we may pull in tabular data that exists in the wild and quickly make features and graphs that allow semantic and topological exploration and traversals. \n",
    "\n",
    "In this way one can quickly track a variety of datasets and (in this case) gauge growth, investment, and promise fullfillment and transparently using Graph Thinking and analysis.\n",
    "\n",
    "Encoding text, categorical, and numeric features while exploring the relationships can be time consuming tasks. We hope that Graphistry[ai] demonstrates an exciting and visually compelling way to explore Graph Data. \n",
    "\n",
    "Now you can mix and match features, augment it with more columns via enrichment, and pivot large amounts of data using natural language search, all using a few lines of code. The features produced may then be used in downstream models, whose outputs could be added and the entire process repeated.\n",
    "\n",
    "Let us know what you think!\n",
    "\n",
    "Join our Slack: Graphistry-Community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af2311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
