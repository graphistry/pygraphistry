---
# Mount the instance store as /tmp. The instance store is a SSD physically attached to the ec2
# server running our instance. It is fast, and free for many instance types, but it is deleted every
# time we shut down our instance.

- name: Checking if we should remount /tmp
  set_fact: remount_tmp=True
  when: '"/mnt" in ansible_mounts|map(attribute="mount")'
  notify:
      - Reboot server
      - Wait for server to restart

# Force handlers to run, which will cause a reboot here if one is to be performed
# - meta: flush_handlers

- name: Change permissions of /mnt so that /tmp has right permissions on mount
  file: path=/mnt
        state=directory
        owner=root
        group=root
        mode=1777
  become: yes
  when: remount_tmp

- name: Unmount the instance store at /mnt
  mount: name=/mnt
         src=/dev/xvdb
         fstype=ext3
         state=unmounted
  become: yes
  when: remount_tmp

- name: Move /tmp to /tmp_old
  shell: mv /tmp /tmp_old
         executable=/bin/bash
  become: yes
  when: remount_tmp

- name: Create /tmp mount point
  file: path=/tmp
        state=directory
        owner=root
        group=root
        mode=1777
  become: yes

- name: Add fstab entry to mount instance store at /tmp
  mount: name=/tmp
         src=/dev/xvdb
         fstype=ext3
         opts="defaults,nobootwait,acl,nofail,barrier=0,noatime,nodiratime,comment=cloudconfig"
         dump=0
         passno=2
         state=mounted
  become: yes
  when: remount_tmp

- name: Move contents of /tmp_old back to /tmp (it's OK if this fails)
  shell: mv /tmp_old/* /tmp/
         executable=/bin/bash
  ignore_errors: true
  become: yes
  when: remount_tmp

- name: Remove /tmp_old
  file: path=/tmp_old
        state=absent
  become: yes

# When we connect over SSH, the SSH Agent forwarding socket is created in /tmp. When we remount
# /tmp, this socket is deleted (sockets can't be moved between drives, so the `mv` command doesn't
# keep it).
# Because we have SSH ControlMaster enabled in our ansible.cfg's ssh_args, Ansible doesn't reconnect
# for each task, and so that SSH agent socket isn't recreated. The below task attempts to manually
# send a ControlMaster `exit` command to SSH, though it may fail if the below SSH options don't
# match Ansible's exactly. On the next task, SSH will reconnect and the socket will be recreated.
- name: Disconnect SSH master connection (so that the SSH agent forwarding socket is recreated)
  local_action: command ssh -o ControlMaster=auto -o ControlPath=/tmp/ansible-ssh-%u-%n-%p-%r -o IdentityFile={{ ansible_ssh_private_key_file|default('./tools/ansible_id_rsa.pem') }} -o Port={{ ansible_ssh_port }} -o User={{ ansible_ssh_user }} -O exit {{ ansible_ssh_host|default(inventory_hostname) }}
  become: no
  ignore_errors: yes
  when: remount_tmp
