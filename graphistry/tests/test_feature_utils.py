# python -m unittest
import datetime as dt
import graphistry
import logging
import numpy as np
import pandas as pd
from typing import Any

import pytest
import unittest
import warnings

from graphistry.feature_utils import (
    process_dirty_dataframes,
    process_nodes_dataframes,
    resolve_feature_engine,
    lazy_import_has_min_dependancy,
    lazy_import_has_dependancy_text,
    FastEncoder
)


has_min_dependancy, _ = lazy_import_has_min_dependancy()
has_min_dependancy_text, _, _ = lazy_import_has_dependancy_text()

logger = logging.getLogger(__name__)
warnings.filterwarnings("ignore")
logging.getLogger("graphistry.feature_utils").setLevel(logging.DEBUG)

model_avg_name = (
    "/models/average_word_embeddings_komninos"  # 250mb, fastest vectorizer in transformer models
    #"/models/paraphrase-albert-small-v2"  # 40mb
    #"/models/paraphrase-MiniLM-L3-v2"  # 60mb
)


bad_df = pd.DataFrame(
    {
        "src": [0, 1, 2, 3],
        "dst": [1, 2, 3, 0],
        "colors": [1, 1, 2, 2],
        "list_int": [[1], [2, 3], [4], []],
        "list_str": [["x"], ["1", "2"], ["y"], []],
        "list_bool": [[True], [True, False], [False], []],
        # "list_date_str": [
        #     ["2018-01-01 00:00:00"],
        #     ["2018-01-02 00:00:00", "2018-01-03 00:00:00"],
        #     ["2018-01-05 00:00:00"],
        #     [],
        # ],
        # "list_date": [
        #     [pd.Timestamp("2018-01-05")],
        #     [pd.Timestamp("2018-01-05"), pd.Timestamp("2018-01-05")],
        #     [],
        #     [],
        # ],
        "list_mixed": [[1], ["1", "2"], [False, None], []],
        "bool": [True, False, True, True],
        "char": ["a", "b", "c", "d"],
        "str": ["a", "b", "c", "d"],
        "ustr": ["a", "b", "c", "d"],
        "emoji": ["ðŸ˜‹", "ðŸ˜‹ðŸ˜‹", "ðŸ˜‹", "ðŸ˜‹"],
        "int": [0, 1, 2, 3],
        "num": [0.5, 1.5, 2.5, 3.5],
        # "date_str": [
        #     "2018-01-01 00:00:00",
        #     "2018-01-02 00:00:00",
        #     "2018-01-03 00:00:00",
        #     "2018-01-05 00:00:00",
        # ],
        # API 1 BUG: Try with https://github.com/graphistry/pygraphistry/pull/126
        # "date": [
        #     dt.datetime(2018, 1, 1),
        #     dt.datetime(2018, 1, 1),
        #     dt.datetime(2018, 1, 1),
        #     dt.datetime(2018, 1, 1),
        # ],
        # "time": [
        #     pd.Timestamp("2018-01-05"),
        #     pd.Timestamp("2018-01-05"),
        #     pd.Timestamp("2018-01-05"),
        #     pd.Timestamp("2018-01-05"),
        # ],
        # # API 2 BUG: Need timedelta in https://github.com/graphistry/pygraphistry/blob/master/graphistry/vgraph.py#L108
        # "delta": [
        #     pd.Timedelta("1 day"),
        #     pd.Timedelta("1 day"),
        #     pd.Timedelta("1 day"),
        #     pd.Timedelta("1 day"),
        # ],
        "textual": [
            "here we have a sentence. And here is another sentence. Graphistry is an amazing tool!"
        ] * 2 + ['And now for something completely different so we dont mess up the tests with a repeat document'] + ['I love my wife'],
    }
)

# ###############################################
# For EDGE FEATURIZATION AND TESTS
edge_df = bad_df.astype(str)
good_edge_cols = [
    "textual",
    #"delta",
    #"time",
    "colors",
    "list_str",
    "bool",
    "char",
    #"list_date",
]

single_target_edge = pd.DataFrame({"emoji": edge_df["emoji"].values})
double_target_edge = pd.DataFrame(
    {"emoji": edge_df["emoji"].values, "num": edge_df["num"].values}
)
target_names_edge = [['emoji'], ['emoji', 'num']]

# ###############################################
# For NODE FEATURIZATION AND TESTS
# data to test textual and meta DataFrame
ndf_reddit = pd.read_csv("graphistry/tests/data/reddit.csv", index_col=0)

text_cols_reddit = ["title", "document"]
meta_cols_reddit = ["user", "type", "label"]
good_cols_reddit = text_cols_reddit + meta_cols_reddit

#test sending in names for target
target_names_node = [['label'], ['label', 'type']]
# test also sending in a dataframe for target
double_target_reddit = pd.DataFrame(
    {"label": ndf_reddit.label.values, "type": ndf_reddit["type"].values}
)
single_target_reddit = pd.DataFrame({"label": ndf_reddit.label.values})

edge_df2 = ndf_reddit[['title', 'label']]
edge_df2['src'] = np.random.random_integers(0, 120, size=len(edge_df2))
edge_df2['dst'] = np.random.random_integers(0, 120, size=len(edge_df2))
edge2_target_df = pd.DataFrame({'label': edge_df2.label})

# ################################################
# data to test textual and numeric DataFrame
# ndf_stocks, price_df_stocks = get_stocks_dataframe()

def allclose_stats(X, x, tol, name):
    if not np.allclose(X.std(), x.std(), tol):
        print(f'{name}.std() are not aligned at {tol} tolerance...!')

    if not np.allclose(X.mean(), x.mean(), tol):
        print(f'{name}.means() are not aligned at {tol} tolerance...!')

    if not np.allclose(X, x, tol):
        print(f'{name}s are not aligned at {tol} tolerance...!')


# so we can use on any two fields, not just x, y
def check_allclose_fit_transform_on_same_data(X, x, Y=None, y=None):
    tols = [100, 10, 0.1, 1e-4, 1e-5]
    for name, tol in zip(['Features', 'Target'], [tols, tols]):
        for value in tol:
            if name == 'Features':
                allclose_stats(X, x, value, name)
            if name == 'Target' and Y is not None and y is not None:
                allclose_stats(Y, y, value, name)


class TestFastEncoder(unittest.TestCase):
    # we test how far off the fit returned values different from the transformed
    
    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def setUp(self):
        fenc = FastEncoder(ndf_reddit, y=double_target_reddit, kind='nodes')
        fenc.fit(feature_engine=resolve_feature_engine('auto'),
                 use_ngrams=True, ngram_range=(1, 1), use_scaler='robust', cardinality_threshold=100)
        self.X, self.Y = fenc.X, fenc.y
        self.x, self.y = fenc.transform(ndf_reddit, ydf=double_target_reddit)

        fenc = FastEncoder(edge_df2, y=edge2_target_df, kind='edges')
        fenc.fit(src='src', dst='dst', feature_engine=resolve_feature_engine('auto'),
                 use_ngrams=True, ngram_range=(1, 1),
                 use_scaler=None,
                 use_scaler_target=None,
                 cardinality_threshold=2, n_topics=4)
        
        self.Xe, self.Ye = fenc.X, fenc.y
        self.xe, self.ye = fenc.transform(edge_df2, ydf=edge2_target_df)
        
    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def test_allclose_fit_transform_on_same_data(self):
        check_allclose_fit_transform_on_same_data(self.X, self.x, self.Y, self.y)
        check_allclose_fit_transform_on_same_data(self.Xe, self.xe, self.Ye, self.ye)
        
    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def test_columns_match(self):
        assert all(self.X.columns == self.x.columns), 'Node Feature Columns do not match'
        assert all(self.Y.columns == self.y.columns), 'Node Target Columns do not match'
        assert all(self.Xe.columns == self.xe.columns), 'Edge Feature Columns do not match'
        assert all(self.Ye.columns == self.ye.columns), 'Edge Target Columns do not match'
        
        
class TestFeatureProcessors(unittest.TestCase):
    def cases_tests(self, x, y, data_encoder, target_encoder, name, value):
        import dirty_cat
        self.assertIsInstance(
            x,
            pd.DataFrame,
            f"Returned data matrix is not Pandas DataFrame for {name} {value}",
        )
        self.assertFalse(
            x.empty,
            f"Pandas DataFrame should not be empty for {name} {value}",
        )
        self.assertIsInstance(
            y,
            pd.DataFrame,
            f"Returned Target is not a Pandas DataFrame for {name} {value}",
        )
        self.assertFalse(
            y.empty,
            f"Pandas Target DataFrame should not be empty for {name} {value}",
        )
        self.assertIsInstance(
            data_encoder,
            dirty_cat.super_vectorizer.SuperVectorizer,
            f"Data Encoder is not a dirty_cat.super_vectorizer.SuperVectorizer instance for {name} {value}",
        )
        self.assertIsInstance(
            target_encoder,
            dirty_cat.super_vectorizer.SuperVectorizer,
            f"Data Target Encoder is not a dirty_cat.super_vectorizer.SuperVectorizer instance for {name} {value}",
        )

    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def test_process_node_dataframes_min_words(self):
        # test different target cardinality
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)
            for min_words in [
                2,
                4000,
            ]:  # last one should skip encoding, and throw all to dirty_cat
                X_enc, y_enc, data_encoder, label_encoder, ordinal_pipeline, ordinal_pipeline_target, text_model, text_cols = process_nodes_dataframes(
                    ndf_reddit,
                    y=double_target_reddit,
                    use_scaler=None,
                    cardinality_threshold=40,
                    cardinality_threshold_target=40,
                    n_topics=20,
                    min_words=min_words,
                    model_name=model_avg_name,
                    feature_engine=resolve_feature_engine('auto')
                )
                self.cases_tests(X_enc, y_enc, data_encoder, label_encoder, "min_words", min_words)
    
    @pytest.mark.skipif(not has_min_dependancy, reason="requires minimal feature dependencies")
    def test_multi_label_binarizer(self):
        g = graphistry.nodes(bad_df)  # can take in a list of lists and convert to multiOutput
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)
            g2 = g.featurize(y=['list_str'], X=['src'], multilabel=True)
        y = g2._get_target('node')
        assert y.shape == (4, 4)
        assert sum(y.sum(1).values - np.array([1., 2., 1., 0.])) == 0
        
class TestFeatureMethods(unittest.TestCase):

    def _check_attributes(self, g, attributes):
        msg = "Graphistry instance after featurization should have `{}` as attribute"
        for attribute in attributes:
            self.assertTrue(hasattr(g, attribute), msg.format(attribute))
            if 'features' in attribute:
                self.assertIsInstance(getattr(g, attribute), pd.DataFrame, msg.format(attribute))
            if 'target' in attribute:
                self.assertIsInstance(getattr(g, attribute), pd.DataFrame, msg.format(attribute))
            if 'encoder' in attribute:
                self.assertIsInstance(getattr(g, attribute), FastEncoder, msg.format(attribute))

    def cases_check_node_attributes(self, g):
        attributes = [
            "_node_features",
            "_node_target",
            "_node_encoder",
        ]
        self._check_attributes(g, attributes)

    def cases_check_edge_attributes(self, g):
        attributes = [
            "_edge_features",
            "_edge_target",
            "_edge_encoder"
        ]
        self._check_attributes(g, attributes)

    def cases_test_graph(self, g, name, value, kind="nodes", df=ndf_reddit):
        print(f'<{name} test graph: {value}>')
        if kind == "nodes":
            ndf = g._nodes
            self.cases_check_node_attributes(g)
        else:
            ndf = g._edges
            self.cases_check_edge_attributes(g)

        cols = ndf.columns
        self.assertTrue(
            np.all(ndf == df[cols]),
            f"Graphistry {kind}-dataframe does not match outside dataframe it was fed",
        )

    def _test_featurizations(self, g, use_cols, targets, name, kind, df):
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)
            for cardinality in [2, 200]:
                for use_ngram in [True, False]:
                    for use_col in use_cols:
                        for target in targets:
                            logger.debug("*" * 90)
                            value = [cardinality, use_ngram, target, use_col]
                            names = "cardinality, use_ngram, target, use_col".split(', ')
                            logger.debug(f"{value}")
                            print(f"{[k for k in zip(names, value)]}")
                            logger.debug("-" * 80)
                            if kind == 'edges' and cardinality == 2:
                                # GapEncoder is set to fail on small documents like our edge_df..., so we skip
                                continue
                            g2 = g.featurize(
                                kind=kind,
                                X=use_col,
                                y=target,
                                model_name=model_avg_name,
                                use_scaler=None,
                                use_scaler_target=None,
                                use_ngrams=use_ngram,
                                min_df=0,
                                max_df=1.,
                                cardinality_threshold=cardinality,
                                cardinality_threshold_target=cardinality
                            )
            
                            self.cases_test_graph(g2, name=name, value=value, kind=kind, df=df)
                                
                
    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def test_node_featurizations(self):
        g = graphistry.nodes(ndf_reddit)
        use_cols = [None, text_cols_reddit, meta_cols_reddit]
        targets = [None, single_target_reddit, double_target_reddit] + target_names_node
        self._test_featurizations(
            g,
            use_cols=use_cols,
            targets=targets,
            name="Node featurization with `(target, use_col)=`",
            kind="nodes",
            df=ndf_reddit,
        )
        

    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def test_edge_featurization(self):
        g = graphistry.edges(edge_df, "src", "dst")
        targets = [None, single_target_edge, double_target_edge] + target_names_edge
        use_cols = [None, good_edge_cols]
        self._test_featurizations(
            g,
            use_cols=use_cols,
            targets=targets,
            name="Edge featurization with `(target, use_col)=`",
            kind="edges",
            df=edge_df,
        )
        
    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def test_node_scaling(self):
        g = graphistry.nodes(ndf_reddit)
        g2 = g.featurize(X="title", y='label', use_scaler=None, use_scaler_target=None)
        scalers = ['quantile', 'zscale', 'kbins', 'robust', 'minmax']
        for scaler in scalers:
            a, b, c, d = g2.scale(ndf_reddit, single_target_reddit, kind='nodes', use_scaler=scaler, use_scaler_target=np.random.choice(scalers))

        

    @pytest.mark.skipif(not has_min_dependancy or not has_min_dependancy_text, reason="requires ai feature dependencies")
    def test_edge_scaling(self):
        g = graphistry.edges(edge_df2, "src", "dst")
        g2 = g.featurize(y='label', kind='edges', use_scaler=None, use_scaler_target=None)
        scalers = ['quantile', 'zscale', 'kbins', 'robust', 'minmax']
        for scaler in scalers:
            a, b, c, d = g2.scale(edge_df2, edge2_target_df, kind='edges', use_scaler=scaler, use_scaler_target=np.random.choice(scalers))



if __name__ == "__main__":
    unittest.main()
